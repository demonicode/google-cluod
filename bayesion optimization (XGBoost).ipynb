{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing the data again "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do the prepprocessing of the data as done on the previous part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Supress unnecessary warnings so that presentation looks clean\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#importing the  necessary modules\n",
    "import pandas                                      #to read and manipulate data\n",
    "import zipfile                                     #to extract data\n",
    "import numpy as np                                 #for matrix operations\n",
    "#read will be imported as and when required\n",
    "#read the train and test zip file\n",
    "zip_ref = zipfile.ZipFile(\"train.csv.zip\", 'r')    \n",
    "zip_ref.extractall()                               \n",
    "zip_ref.close()\n",
    "\n",
    "train_data = pandas.read_csv(\"train.csv\")\n",
    "\n",
    "import copy\n",
    "test_data = copy.deepcopy(train_data.iloc[150000:])\n",
    "train_data = train_data.iloc[:150000]\n",
    "\n",
    "y_true = test_data['loss']\n",
    "\n",
    "ids = test_data['id']\n",
    "\n",
    "target = train_data['loss']\n",
    "\n",
    "#drop the unnecessary column id and loss from both train and test set.\n",
    "train_data.drop(['id','loss'],1,inplace=True)\n",
    "test_data.drop(['id','loss'],1,inplace=True)\n",
    "\n",
    "shift = 200\n",
    "target = np.log(target+shift)\n",
    "\n",
    "#merging both the datasets to make single joined dataset\n",
    "joined = pandas.concat([train_data, test_data],ignore_index = True)\n",
    "del train_data,test_data                                         #deleting previous one to save memory.\n",
    "\n",
    "cat_feature = [n for n in joined.columns if n.startswith('cat')]  #list of all the features containing categorical values\n",
    "\n",
    "#factorizing them\n",
    "for column in cat_feature:\n",
    "    joined[column] = pandas.factorize(joined[column].values, sort=True)[0]\n",
    "        \n",
    "del cat_feature\n",
    "\n",
    "#dividing the training data between training and testing set\n",
    "train_data = joined.iloc[:150000,:]\n",
    "test_data = joined.iloc[150000:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/demonicode/.local/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#inporting additional files\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from bayes_opt import BayesianOptimization\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#making the function to be used for different values of hyper-parameter\n",
    "def xgb_evaluate(min_child_weight,colsample_bytree,max_depth,subsample,gamma,alpha):\n",
    "    params['min_child_weight'] = int(min_child_weight)\n",
    "    params['cosample_bytree'] = max(min(colsample_bytree, 1), 0)\n",
    "    params['max_depth'] = int(max_depth)\n",
    "    params['subsample'] = max(min(subsample, 1), 0)\n",
    "    params['gamma'] = max(gamma, 0)\n",
    "    params['alpha'] = max(alpha, 0)\n",
    "\n",
    "    cv_result = xgb.cv(params, xgtrain, num_boost_round=num_rounds, nfold=5,seed=random_state,\n",
    "             callbacks=[xgb.callback.early_stop(50)])\n",
    "    \n",
    "    #returning negative of cv result since, xgb.cv maximizes the score and we need to minimize the error\n",
    "    return -cv_result['test-mae-mean'].values[-1]              \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#making xgb matrix to be used as input\n",
    "xgtrain = xgb.DMatrix(train_data, label=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#although after running a couple of iterations , it was seen that num_rounds never crossed even 1000, but \\n#to not take a risk, we set it to 3000\\nnum_rounds = 3000\\n#random seed value to make the results replicable\\nrandom_state = 2016\\nnum_iter = 25\\ninit_points = 5\\nparams = {'eta': 0.1,'silent': 1,'eval_metric': 'mae','verbose_eval': 2,'seed': random_state}\\n\\nxgbBO = BayesianOptimization(xgb_evaluate, {'min_child_weight': (1, 20),\\n                                                'colsample_bytree': (0.1, 1),\\n                                                'max_depth': (5, 15),\\n                                                'subsample': (0.5, 1),\\n                                                'gamma': (0, 10),\\n                                                'alpha': (0, 10),\\n                                                })\\n\\n#using bayesian optimization to maximize the passed score like accuracy\\nxgbBO.maximize(init_points=init_points, n_iter=num_iter)\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#although after running a couple of iterations , it was seen that num_rounds crossed even 2000, So \n",
    "#to not take a risk, we set it to 3000\n",
    "num_rounds = 3000\n",
    "#random seed value to make the results replicable\n",
    "random_state = 2016\n",
    "num_iter = 25\n",
    "init_points = 5\n",
    "params = {'eta': 0.1,'silent': 1,'eval_metric': 'mae','verbose_eval': 2,'seed': random_state}\n",
    "\n",
    "xgbBO = BayesianOptimization(xgb_evaluate, {'min_child_weight': (1, 20),\n",
    "                                                'colsample_bytree': (0.1, 1),\n",
    "                                                'max_depth': (5, 15),\n",
    "                                                'subsample': (0.5, 1),\n",
    "                                                'gamma': (0, 10),\n",
    "                                                'alpha': (0, 10),\n",
    "                                                })\n",
    "\n",
    "#using bayesian optimization to maximize the passed score like accuracy\n",
    "xgbBO.maximize(init_points=init_points, n_iter=num_iter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, it is a computationally heavy task to run model so many times, i couldn't do it on my laptop even after running for hours, therefore, i used google cloud to run this code as a script. Even on a 8 core 50 gb ram instance of google cloud, you can get an idea the time it was taking to complete, I'm posting the results below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Initialization\n",
    "---------------------------------------------------------------------------------------------------------------------------\n",
    " Step |   Time |      Value |     alpha |   colsample_bytree |     gamma |   max_depth |   min_child_weight |   subsample | \n",
    "Multiple eval metrics have been passed: 'test-mae' will be used for early stopping.\n",
    "\n",
    "Will train until test-mae hasn't improved in 50 rounds.\n",
    "Stopping. Best iteration:\n",
    "[995]   train-mae:0.353615+0.000552759  test-mae:0.37291+0.00241128\n",
    "\n",
    "    1 | 38m51s |   -0.37291 |    4.4158 |             0.9119 |    2.4020 |     14.5118 |            10.6760 |      0.5503 | \n",
    "Multiple eval metrics have been passed: 'test-mae' will be used for early stopping.\n",
    "\n",
    "Will train until test-mae hasn't improved in 50 rounds.\n",
    "Stopping. Best iteration:\n",
    "[566]   train-mae:0.379244+0.000735928  test-mae:0.38179+0.00192141\n",
    "\n",
    "    2 | 10m22s |   -0.38179 |    7.4460 |             0.9433 |    9.5554 |      7.4645 |            18.5685 |      0.6715 | \n",
    "Multiple eval metrics have been passed: 'test-mae' will be used for early stopping.\n",
    "\n",
    "Will train until test-mae hasn't improved in 50 rounds.\n",
    "Stopping. Best iteration:\n",
    "[1669]  train-mae:0.374388+0.00059465   test-mae:0.378342+0.0019866\n",
    "\n",
    "    3 | 46m57s |   -0.37834 |    8.0450 |             0.5499 |    7.9558 |     12.3267 |            19.2943 |      0.9618 | \n",
    "Multiple eval metrics have been passed: 'test-mae' will be used for early stopping.\n",
    "\n",
    "Will train until test-mae hasn't improved in 50 rounds.\n",
    "Stopping. Best iteration:\n",
    "[2728]  train-mae:0.371039+0.000594925  test-mae:0.376165+0.0022578\n",
    "\n",
    "    4 | 31m21s |   -0.37616 |    9.9794 |             0.8270 |    3.5062 |      5.0242 |             7.3661 |      0.7791 | \n",
    "Multiple eval metrics have been passed: 'test-mae' will be used for early stopping.\n",
    "\n",
    "Will train until test-mae hasn't improved in 50 rounds.\n",
    "Stopping. Best iteration:\n",
    "[1003]  train-mae:0.361415+0.000529711  test-mae:0.37304+0.00236389\n",
    "\n",
    "    5 | 21m30s |   -0.37304 |    6.8519 |             0.8878 |    2.4687 |     10.0548 |             7.1863 |      0.9971 | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it took quite some time as you can see from time column and after that the SSH connection was broken. Since, it did give an idea of what paratmeters to try like max_depth should be in range 10-15 and alpha less that 5, col_sample bytree around 0.8 and gamma around 2,min_child weight 10 or less,subsample, not so much so have to try that between 0.5 to 1\n",
    "Next, we would use grid search to find exact parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE- this code is taken from Vladimir Iglovikov github repository. Link - https://github.com/fmfn/BayesianOptimization/blob/master/examples/xgboost_example.py\n",
    "\n",
    "Minor modifications were also tried but this provides the best results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
