{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import zipfile\n",
    "import numpy as np\n",
    "zip_ref = zipfile.ZipFile(\"train.csv.zip\", 'r')\n",
    "zip_ref.extractall()\n",
    "zip_ref.close()\n",
    "zip_ref2 = zipfile.ZipFile(\"test.csv.zip\", 'r')\n",
    "zip_ref2.extractall()\n",
    "zip_ref2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pandas.read_csv(\"train.csv\")\n",
    "test_data = pandas.read_csv(\"test.csv\")\n",
    "test_data['loss'] = np.nan\n",
    "ids = test_data['id']\n",
    "\n",
    "test_data.drop(\"id\", 1,inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat1</th>\n",
       "      <th>cat2</th>\n",
       "      <th>cat3</th>\n",
       "      <th>cat4</th>\n",
       "      <th>cat5</th>\n",
       "      <th>cat6</th>\n",
       "      <th>cat7</th>\n",
       "      <th>cat8</th>\n",
       "      <th>cat9</th>\n",
       "      <th>cat10</th>\n",
       "      <th>...</th>\n",
       "      <th>cont6</th>\n",
       "      <th>cont7</th>\n",
       "      <th>cont8</th>\n",
       "      <th>cont9</th>\n",
       "      <th>cont10</th>\n",
       "      <th>cont11</th>\n",
       "      <th>cont12</th>\n",
       "      <th>cont13</th>\n",
       "      <th>cont14</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0.718367</td>\n",
       "      <td>0.335060</td>\n",
       "      <td>0.30260</td>\n",
       "      <td>0.67135</td>\n",
       "      <td>0.83510</td>\n",
       "      <td>0.569745</td>\n",
       "      <td>0.594646</td>\n",
       "      <td>0.822493</td>\n",
       "      <td>0.714843</td>\n",
       "      <td>2213.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.438917</td>\n",
       "      <td>0.436585</td>\n",
       "      <td>0.60087</td>\n",
       "      <td>0.35127</td>\n",
       "      <td>0.43919</td>\n",
       "      <td>0.338312</td>\n",
       "      <td>0.366307</td>\n",
       "      <td>0.611431</td>\n",
       "      <td>0.304496</td>\n",
       "      <td>1283.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289648</td>\n",
       "      <td>0.315545</td>\n",
       "      <td>0.27320</td>\n",
       "      <td>0.26076</td>\n",
       "      <td>0.32446</td>\n",
       "      <td>0.381398</td>\n",
       "      <td>0.373424</td>\n",
       "      <td>0.195709</td>\n",
       "      <td>0.774425</td>\n",
       "      <td>3005.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>...</td>\n",
       "      <td>0.440945</td>\n",
       "      <td>0.391128</td>\n",
       "      <td>0.31796</td>\n",
       "      <td>0.32128</td>\n",
       "      <td>0.44467</td>\n",
       "      <td>0.327915</td>\n",
       "      <td>0.321570</td>\n",
       "      <td>0.605077</td>\n",
       "      <td>0.602642</td>\n",
       "      <td>939.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>B</td>\n",
       "      <td>...</td>\n",
       "      <td>0.178193</td>\n",
       "      <td>0.247408</td>\n",
       "      <td>0.24564</td>\n",
       "      <td>0.22089</td>\n",
       "      <td>0.21230</td>\n",
       "      <td>0.204687</td>\n",
       "      <td>0.202213</td>\n",
       "      <td>0.246011</td>\n",
       "      <td>0.432606</td>\n",
       "      <td>2763.85</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9 cat10   ...        cont6  \\\n",
       "0    A    B    A    B    A    A    A    A    B     A   ...     0.718367   \n",
       "1    A    B    A    A    A    A    A    A    B     B   ...     0.438917   \n",
       "2    A    B    A    A    B    A    A    A    B     B   ...     0.289648   \n",
       "3    B    B    A    B    A    A    A    A    B     A   ...     0.440945   \n",
       "4    A    B    A    B    A    A    A    A    B     B   ...     0.178193   \n",
       "\n",
       "      cont7    cont8    cont9   cont10    cont11    cont12    cont13  \\\n",
       "0  0.335060  0.30260  0.67135  0.83510  0.569745  0.594646  0.822493   \n",
       "1  0.436585  0.60087  0.35127  0.43919  0.338312  0.366307  0.611431   \n",
       "2  0.315545  0.27320  0.26076  0.32446  0.381398  0.373424  0.195709   \n",
       "3  0.391128  0.31796  0.32128  0.44467  0.327915  0.321570  0.605077   \n",
       "4  0.247408  0.24564  0.22089  0.21230  0.204687  0.202213  0.246011   \n",
       "\n",
       "     cont14     loss  \n",
       "0  0.714843  2213.18  \n",
       "1  0.304496  1283.60  \n",
       "2  0.774425  3005.09  \n",
       "3  0.602642   939.85  \n",
       "4  0.432606  2763.85  \n",
       "\n",
       "[5 rows x 131 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.drop(\"id\",axis = 1,inplace = True)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(188318, 131)\n",
      "(125546, 131)\n"
     ]
    }
   ],
   "source": [
    "print (train_data.shape)\n",
    "print (test_data.shape)\n",
    "X_train_end = train_data.shape[0]-1\n",
    "X_test_start = train_data.shape[0]\n",
    "X_test_end = train_data.shape[0] + test_data.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = pandas.concat([train_data, test_data],ignore_index = True)\n",
    "del train_data,test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       cat1 cat2 cat3 cat4 cat5 cat6 cat7 cat8 cat9 cat10    ...        cont6  \\\n",
      "0         A    B    A    B    A    A    A    A    B     A    ...     0.718367   \n",
      "1         A    B    A    A    A    A    A    A    B     B    ...     0.438917   \n",
      "2         A    B    A    A    B    A    A    A    B     B    ...     0.289648   \n",
      "3         B    B    A    B    A    A    A    A    B     A    ...     0.440945   \n",
      "4         A    B    A    B    A    A    A    A    B     B    ...     0.178193   \n",
      "5         A    B    A    A    A    A    A    A    B     A    ...     0.364464   \n",
      "6         A    A    A    A    B    A    A    A    A     A    ...     0.381515   \n",
      "7         A    B    A    B    A    A    A    A    B     A    ...     0.867021   \n",
      "8         A    B    B    B    B    A    A    A    B     B    ...     0.628534   \n",
      "9         A    B    A    A    B    B    A    A    B     A    ...     0.713343   \n",
      "10        A    B    A    A    A    A    A    A    B     B    ...     0.429383   \n",
      "11        A    B    A    A    B    A    A    A    B     B    ...     0.314683   \n",
      "12        B    A    A    A    B    A    A    A    A     A    ...     0.408772   \n",
      "13        B    A    A    A    B    B    A    A    A     A    ...     0.241574   \n",
      "14        A    A    A    A    B    A    A    A    A     A    ...     0.894903   \n",
      "15        A    A    A    A    B    B    A    A    A     A    ...     0.570733   \n",
      "16        A    B    B    A    A    A    A    A    B     B    ...     0.411902   \n",
      "17        A    A    A    A    A    B    A    A    A     A    ...     0.688705   \n",
      "18        A    A    B    A    A    B    A    A    A     A    ...     0.443265   \n",
      "19        A    A    A    B    A    A    A    A    A     A    ...     0.436312   \n",
      "20        B    B    A    B    A    A    A    A    B     A    ...     0.441525   \n",
      "21        A    A    A    B    A    B    A    A    A     A    ...     0.349885   \n",
      "22        B    A    A    A    B    B    A    A    A     A    ...     0.183243   \n",
      "23        B    A    A    B    A    A    A    A    A     A    ...     0.373500   \n",
      "24        B    A    A    A    A    A    A    A    A     A    ...     0.382070   \n",
      "25        A    A    A    B    A    A    A    A    A     A    ...     0.592478   \n",
      "26        A    A    A    A    A    B    A    A    A     A    ...     0.435733   \n",
      "27        B    A    A    B    A    A    A    A    A     A    ...     0.373500   \n",
      "28        A    B    A    B    A    B    A    A    B     A    ...     0.671307   \n",
      "29        A    A    A    A    B    A    A    A    A     A    ...     0.557431   \n",
      "...     ...  ...  ...  ...  ...  ...  ...  ...  ...   ...    ...          ...   \n",
      "313834    A    B    A    A    A    A    A    B    B     A    ...     0.698205   \n",
      "313835    A    B    A    B    B    A    A    A    B     B    ...     0.862482   \n",
      "313836    A    B    A    A    B    A    A    A    B     A    ...     0.340845   \n",
      "313837    B    A    A    A    A    B    A    A    A     A    ...     0.458113   \n",
      "313838    B    B    A    A    A    A    A    A    B     B    ...     0.350956   \n",
      "313839    A    A    A    A    A    B    A    A    A     A    ...     0.688705   \n",
      "313840    A    A    A    A    A    B    A    A    A     A    ...     0.535867   \n",
      "313841    A    B    A    A    A    B    A    A    B     A    ...     0.421908   \n",
      "313842    A    A    A    A    A    B    A    A    A     A    ...     0.646468   \n",
      "313843    A    A    A    A    B    A    A    A    A     A    ...     0.271144   \n",
      "313844    A    A    A    A    A    B    A    A    A     A    ...     0.364464   \n",
      "313845    A    A    A    B    B    A    A    A    A     A    ...     0.487419   \n",
      "313846    A    A    A    A    A    B    A    A    A     A    ...     0.867697   \n",
      "313847    A    A    B    A    A    A    A    A    A     A    ...     0.431689   \n",
      "313848    A    B    A    A    B    A    A    A    B     A    ...     0.356602   \n",
      "313849    A    B    A    A    A    A    A    A    B     A    ...     0.571597   \n",
      "313850    A    A    A    B    B    A    B    A    A     A    ...     0.767863   \n",
      "313851    A    A    A    B    A    A    A    A    A     A    ...     0.731035   \n",
      "313852    A    B    A    A    B    A    A    A    B     A    ...     0.435733   \n",
      "313853    B    A    A    A    B    B    A    A    A     A    ...     0.393798   \n",
      "313854    B    A    A    A    B    A    A    A    A     A    ...     0.730804   \n",
      "313855    A    A    A    B    A    A    A    A    A     A    ...     0.597862   \n",
      "313856    A    B    A    A    B    A    A    A    B     A    ...     0.438917   \n",
      "313857    A    A    A    B    A    A    A    A    A     A    ...     0.189484   \n",
      "313858    A    A    B    A    A    B    A    A    A     A    ...     0.375429   \n",
      "313859    A    A    A    B    A    A    A    A    A     A    ...     0.438917   \n",
      "313860    A    A    A    A    B    B    A    B    A     A    ...     0.346948   \n",
      "313861    B    B    A    A    B    A    A    A    B     B    ...     0.808958   \n",
      "313862    A    A    A    A    A    B    A    B    A     A    ...     0.372125   \n",
      "313863    A    B    A    A    A    A    A    A    B     A    ...     0.221699   \n",
      "\n",
      "           cont7    cont8    cont9   cont10    cont11    cont12    cont13  \\\n",
      "0       0.335060  0.30260  0.67135  0.83510  0.569745  0.594646  0.822493   \n",
      "1       0.436585  0.60087  0.35127  0.43919  0.338312  0.366307  0.611431   \n",
      "2       0.315545  0.27320  0.26076  0.32446  0.381398  0.373424  0.195709   \n",
      "3       0.391128  0.31796  0.32128  0.44467  0.327915  0.321570  0.605077   \n",
      "4       0.247408  0.24564  0.22089  0.21230  0.204687  0.202213  0.246011   \n",
      "5       0.401162  0.26847  0.46226  0.50556  0.366788  0.359249  0.345247   \n",
      "6       0.363768  0.24564  0.40455  0.47225  0.334828  0.352251  0.342239   \n",
      "7       0.583389  0.90267  0.84847  0.80218  0.644013  0.785706  0.859764   \n",
      "8       0.384099  0.61229  0.38249  0.51111  0.682315  0.669033  0.756454   \n",
      "9       0.469223  0.30260  0.67135  0.83510  0.863052  0.879347  0.822493   \n",
      "10      0.877905  0.39455  0.53565  0.50556  0.550529  0.538473  0.336261   \n",
      "11      0.370419  0.58354  0.46226  0.38016  0.644013  0.665644  0.339244   \n",
      "12      0.363312  0.32843  0.32128  0.44467  0.327915  0.321570  0.605077   \n",
      "13      0.255339  0.58934  0.32496  0.26029  0.257148  0.253044  0.276878   \n",
      "14      0.586433  0.80058  0.93383  0.78770  0.880469  0.871011  0.822493   \n",
      "15      0.547756  0.80438  0.44352  0.63026  0.385085  0.377003  0.516660   \n",
      "16      0.593548  0.31796  0.38846  0.48889  0.457203  0.447145  0.301535   \n",
      "17      0.437192  0.67263  0.83505  0.59334  0.678924  0.665644  0.684242   \n",
      "18      0.637086  0.36636  0.52938  0.39068  0.678924  0.665644  0.304350   \n",
      "19      0.544355  0.48864  0.36285  0.20496  0.388786  0.406090  0.648701   \n",
      "20      0.437192  0.31796  0.32128  0.44467  0.377724  0.369858  0.605077   \n",
      "21      0.381185  0.81542  0.32311  0.36458  0.453334  0.454705  0.651733   \n",
      "22      0.253560  0.40028  0.21374  0.19431  0.167024  0.165648  0.404520   \n",
      "23      0.381883  0.36083  0.44352  0.45017  0.338312  0.366307  0.339244   \n",
      "24      0.451203  0.33906  0.47900  0.54433  0.812519  0.800726  0.246011   \n",
      "25      0.496452  0.29758  0.46226  0.51111  0.434083  0.424625  0.357400   \n",
      "26      0.769905  0.60087  0.40252  0.28677  0.550529  0.538473  0.298734   \n",
      "27      0.356037  0.36083  0.44352  0.45017  0.291268  0.295524  0.339244   \n",
      "28      0.464924  0.33906  0.62542  0.66076  0.607500  0.594646  0.678452   \n",
      "29      0.402942  0.34445  0.52728  0.79139  0.377724  0.369858  0.687115   \n",
      "...          ...      ...      ...      ...       ...       ...       ...   \n",
      "313834  0.526532  0.35533  0.38249  0.73971  0.607500  0.594646  0.736269   \n",
      "313835  0.458423  0.29260  0.89888  0.81591  0.550529  0.623714  0.919827   \n",
      "313836  0.347485  0.31280  0.39849  0.41743  0.314313  0.308395  0.351299   \n",
      "313837  0.386882  0.36083  0.46226  0.36458  0.388786  0.380595  0.648701   \n",
      "313838  0.363768  0.58354  0.44352  0.39599  0.341813  0.352251  0.339244   \n",
      "313839  0.480915  0.33906  0.62542  0.73106  0.661688  0.648446  0.687115   \n",
      "313840  0.907969  0.45883  0.52309  0.52221  0.705501  0.692256  0.330336   \n",
      "313841  0.637770  0.34987  0.40657  0.40666  0.468839  0.458493  0.287682   \n",
      "313842  0.411042  0.52450  0.64873  0.79139  0.377724  0.369858  0.689974   \n",
      "313843  0.302709  0.41762  0.37065  0.38541  0.231253  0.241676  0.388569   \n",
      "313844  0.401162  0.26847  0.46226  0.50556  0.415029  0.406090  0.345247   \n",
      "313845  0.394895  0.93736  0.51050  0.43373  0.396226  0.387819  0.633362   \n",
      "313846  0.724409  0.68823  0.71934  0.79863  0.837272  0.826178  0.879390   \n",
      "313847  0.649542  0.33906  0.40455  0.47779  0.689039  0.675759  0.315758   \n",
      "313848  0.338367  0.32843  0.32128  0.36974  0.307628  0.301921  0.608259   \n",
      "313849  0.538203  0.45883  0.49370  0.52775  0.742852  0.729856  0.369740   \n",
      "313850  0.669049  0.94012  0.64103  0.80218  0.745820  0.753252  0.717751   \n",
      "313851  0.994883  0.64577  0.71934  0.62507  0.909611  0.901612  0.354344   \n",
      "313852  0.453404  0.69840  0.39447  0.46119  0.430255  0.519456  0.605077   \n",
      "313853  0.533539  0.27797  0.50420  0.31003  0.678924  0.729856  0.333292   \n",
      "313854  0.994421  0.64577  0.71764  0.62507  0.909611  0.901612  0.354344   \n",
      "313855  0.475866  0.57187  0.73271  0.75234  0.588753  0.576121  0.768525   \n",
      "313856  0.705814  0.77668  0.35127  0.30060  0.569745  0.557380  0.274217   \n",
      "313857  0.265894  0.25918  0.24180  0.21230  0.169206  0.167768  0.339244   \n",
      "313858  0.389249  0.41182  0.42289  0.45017  0.341813  0.335036  0.382252   \n",
      "313859  0.815941  0.39455  0.48740  0.40666  0.550529  0.538473  0.298734   \n",
      "313860  0.424968  0.47669  0.25753  0.26894  0.324486  0.352251  0.490001   \n",
      "313861  0.511502  0.72299  0.94438  0.83510  0.933174  0.926619  0.848129   \n",
      "313862  0.388545  0.31796  0.32128  0.36974  0.307628  0.301921  0.608259   \n",
      "313863  0.242044  0.25461  0.31399  0.25183  0.245410  0.241676  0.287682   \n",
      "\n",
      "          cont14      loss  \n",
      "0       0.714843   2213.18  \n",
      "1       0.304496   1283.60  \n",
      "2       0.774425   3005.09  \n",
      "3       0.602642    939.85  \n",
      "4       0.432606   2763.85  \n",
      "5       0.726792   5142.87  \n",
      "6       0.382931   1132.22  \n",
      "7       0.242416   3585.75  \n",
      "8       0.361191  10280.20  \n",
      "9       0.294523   6184.59  \n",
      "10      0.715009   6396.85  \n",
      "11      0.799124   5965.73  \n",
      "12      0.818358   1193.05  \n",
      "13      0.477578   1071.77  \n",
      "14      0.251278    585.18  \n",
      "15      0.340325   1395.45  \n",
      "16      0.205651   6609.32  \n",
      "17      0.407411   2658.70  \n",
      "18      0.310796   4167.32  \n",
      "19      0.830931   3797.89  \n",
      "20      0.743810   1155.48  \n",
      "21      0.354002    891.14  \n",
      "22      0.725941    765.97  \n",
      "23      0.793518    771.58  \n",
      "24      0.215055   7256.49  \n",
      "25      0.311644   1528.73  \n",
      "26      0.698006   4787.07  \n",
      "27      0.804795   2163.97  \n",
      "28      0.285224  11673.03  \n",
      "29      0.297788   1753.50  \n",
      "...          ...       ...  \n",
      "313834  0.721896       NaN  \n",
      "313835  0.602363       NaN  \n",
      "313836  0.254988       NaN  \n",
      "313837  0.719271       NaN  \n",
      "313838  0.236616       NaN  \n",
      "313839  0.357316       NaN  \n",
      "313840  0.804035       NaN  \n",
      "313841  0.807022       NaN  \n",
      "313842  0.838158       NaN  \n",
      "313843  0.390576       NaN  \n",
      "313844  0.230681       NaN  \n",
      "313845  0.723703       NaN  \n",
      "313846  0.624095       NaN  \n",
      "313847  0.725515       NaN  \n",
      "313848  0.812106       NaN  \n",
      "313849  0.728514       NaN  \n",
      "313850  0.235890       NaN  \n",
      "313851  0.294556       NaN  \n",
      "313852  0.776205       NaN  \n",
      "313853  0.772099       NaN  \n",
      "313854  0.818373       NaN  \n",
      "313855  0.601881       NaN  \n",
      "313856  0.387062       NaN  \n",
      "313857  0.833053       NaN  \n",
      "313858  0.836701       NaN  \n",
      "313859  0.345946       NaN  \n",
      "313860  0.290576       NaN  \n",
      "313861  0.808125       NaN  \n",
      "313862  0.361542       NaN  \n",
      "313863  0.220323       NaN  \n",
      "\n",
      "[313864 rows x 131 columns]\n"
     ]
    }
   ],
   "source": [
    "joined.describe()\n",
    "print joined.iloc[0:X_test_end,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-7183af98414a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/demonicode/anaconda2/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36mstat_func\u001b[0;34m(self, axis, skipna, level, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m   6195\u001b[0m                                       skipna=skipna)\n\u001b[1;32m   6196\u001b[0m         return self._reduce(f, name, axis=axis, skipna=skipna,\n\u001b[0;32m-> 6197\u001b[0;31m                             numeric_only=numeric_only)\n\u001b[0m\u001b[1;32m   6198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6199\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mset_function_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstat_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/demonicode/anaconda2/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_reduce\u001b[0;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[1;32m   5157\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5158\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5159\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5160\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/demonicode/anaconda2/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36mf\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   5146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5147\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5148\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5150\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_agg_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/demonicode/anaconda2/lib/python2.7/site-packages/pandas/core/nanops.pyc\u001b[0m in \u001b[0;36m_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0;31m# we want to transform an object array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print (joined.skew())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#upper = train_data.shape[0]\n",
    "upper = X_test_end\n",
    "train_data = joined.iloc[0:upper,:]\n",
    "#import plotting libraries\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#range of features considered\n",
    "split = 116 \n",
    "\n",
    "#number of features considered\n",
    "size = 15\n",
    "\n",
    "#create a dataframe with only continuous features\n",
    "data=train_data.iloc[:,split:] \n",
    "\n",
    "#get the names of all the columns\n",
    "cols=data.columns \n",
    "\n",
    "data.plot(kind='density', subplots=True, layout=(15,15), sharex=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#import plotting libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#range of features considered\n",
    "split = 116\n",
    "#number of features considered\n",
    "size = 15\n",
    "\n",
    "#create a dataframe with only continuous features\n",
    "data=train_data.iloc[:,split:] \n",
    "#get the names of all the columns\n",
    "cols=data.columns \n",
    "\n",
    "#Plot violin for all attributes in a 7x2 grid\n",
    "n_cols = 2\n",
    "n_rows = 7\n",
    "\n",
    "for i in range(n_rows):\n",
    "    fg,ax = plt.subplots(nrows=1,ncols=n_cols,figsize=(12, 8))\n",
    "    for j in range(n_cols):\n",
    "        sns.violinplot(y=cols[i*n_cols+j], data=train_data, ax=ax[j])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAADnCAYAAAAJiLFaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f93f7027dd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAADqCAYAAABObZUeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f93dc042f10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import copy\n",
    "trial_data = copy.deepcopy(train_data)\n",
    "#print trial_data.iloc[18001+1359,split+size-1]\n",
    "train_data = copy.deepcopy(trial_data)\n",
    "sns.violinplot(data = train_data, y = \"loss\")\n",
    "plt.show()\n",
    "shift = 700\n",
    "train_data[\"loss\"] = np.log(train_data[\"loss\"])\n",
    "sns.violinplot(data = train_data, y = \"loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cont11 and cont12 = 0.99\n",
      "cont1 and cont9 = 0.93\n",
      "cont6 and cont10 = 0.88\n",
      "cont6 and cont13 = 0.81\n",
      "cont1 and cont10 = 0.80\n",
      "cont6 and cont9 = 0.79\n",
      "cont9 and cont10 = 0.78\n",
      "cont6 and cont12 = 0.78\n",
      "cont6 and cont11 = 0.77\n",
      "cont1 and cont6 = 0.76\n",
      "cont7 and cont11 = 0.75\n",
      "cont7 and cont12 = 0.74\n",
      "cont10 and cont12 = 0.71\n",
      "cont10 and cont13 = 0.70\n",
      "cont10 and cont11 = 0.70\n",
      "cont6 and cont7 = 0.65\n",
      "cont9 and cont13 = 0.64\n",
      "cont9 and cont12 = 0.62\n",
      "cont1 and cont12 = 0.61\n",
      "cont9 and cont11 = 0.60\n",
      "cont1 and cont11 = 0.59\n",
      "cont1 and cont13 = 0.53\n",
      "cont4 and cont8 = 0.52\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "data_corr = data.corr()\n",
    "\n",
    "# Set the threshold to select only highly correlated attributes\n",
    "threshold = 0.5\n",
    "\n",
    "# List of pairs along with correlation above threshold\n",
    "corr_list = []\n",
    "\n",
    "#Search for the highly correlated pairs\n",
    "for i in range(0,size): #for 'size' features\n",
    "    for j in range(i+1,size): #avoid repetition\n",
    "        if (data_corr.iloc[i,j] >= threshold and data_corr.iloc[i,j] < 1) or (data_corr.iloc[i,j] < 0 and data_corr.iloc[i,j] <= -threshold):\n",
    "            corr_list.append([data_corr.iloc[i,j],i,j]) #store correlation and columns index\n",
    "\n",
    "#Sort to show higher ones first            \n",
    "s_corr_list = sorted(corr_list,key=lambda x: -abs(x[0]))\n",
    "\n",
    "#Print correlations and column names\n",
    "for v,i,j in s_corr_list:\n",
    "    print (\"%s and %s = %.2f\" % (cols[i],cols[j],v))\n",
    "\n",
    "# Strong correlation is observed between the following pairs\n",
    "# This represents an opportunity to reduce the feature set through transformations such as PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       cat1  cat2  cat3  cat4  cat5  cat6  cat7  cat8  cat9  cat10    ...     \\\n",
      "0         0     1     0     1     0     0     0     0     1      0    ...      \n",
      "1         0     1     0     0     0     0     0     0     1      1    ...      \n",
      "2         0     1     0     0     1     0     0     0     1      1    ...      \n",
      "3         1     1     0     1     0     0     0     0     1      0    ...      \n",
      "4         0     1     0     1     0     0     0     0     1      1    ...      \n",
      "5         0     1     0     0     0     0     0     0     1      0    ...      \n",
      "6         0     0     0     0     1     0     0     0     0      0    ...      \n",
      "7         0     1     0     1     0     0     0     0     1      0    ...      \n",
      "8         0     1     1     1     1     0     0     0     1      1    ...      \n",
      "9         0     1     0     0     1     1     0     0     1      0    ...      \n",
      "10        0     1     0     0     0     0     0     0     1      1    ...      \n",
      "11        0     1     0     0     1     0     0     0     1      1    ...      \n",
      "12        1     0     0     0     1     0     0     0     0      0    ...      \n",
      "13        1     0     0     0     1     1     0     0     0      0    ...      \n",
      "14        0     0     0     0     1     0     0     0     0      0    ...      \n",
      "15        0     0     0     0     1     1     0     0     0      0    ...      \n",
      "16        0     1     1     0     0     0     0     0     1      1    ...      \n",
      "17        0     0     0     0     0     1     0     0     0      0    ...      \n",
      "18        0     0     1     0     0     1     0     0     0      0    ...      \n",
      "19        0     0     0     1     0     0     0     0     0      0    ...      \n",
      "20        1     1     0     1     0     0     0     0     1      0    ...      \n",
      "21        0     0     0     1     0     1     0     0     0      0    ...      \n",
      "22        1     0     0     0     1     1     0     0     0      0    ...      \n",
      "23        1     0     0     1     0     0     0     0     0      0    ...      \n",
      "24        1     0     0     0     0     0     0     0     0      0    ...      \n",
      "25        0     0     0     1     0     0     0     0     0      0    ...      \n",
      "26        0     0     0     0     0     1     0     0     0      0    ...      \n",
      "27        1     0     0     1     0     0     0     0     0      0    ...      \n",
      "28        0     1     0     1     0     1     0     0     1      0    ...      \n",
      "29        0     0     0     0     1     0     0     0     0      0    ...      \n",
      "...     ...   ...   ...   ...   ...   ...   ...   ...   ...    ...    ...      \n",
      "19970     0     1     0     0     0     0     0     1     0      0    ...      \n",
      "19971     0     1     0     0     0     0     0     0     1      0    ...      \n",
      "19972     0     1     0     1     0     0     0     0     1      1    ...      \n",
      "19973     0     1     0     0     0     0     0     0     1      0    ...      \n",
      "19974     0     0     0     0     0     1     0     0     0      0    ...      \n",
      "19975     1     0     0     0     0     1     0     0     0      0    ...      \n",
      "19976     0     1     0     0     1     0     0     0     1      1    ...      \n",
      "19977     0     0     0     0     0     1     0     0     0      0    ...      \n",
      "19978     1     0     0     0     1     1     0     0     0      0    ...      \n",
      "19979     0     0     0     0     0     1     0     0     0      0    ...      \n",
      "19980     0     1     0     0     0     0     0     0     1      0    ...      \n",
      "19981     0     1     0     0     1     0     0     0     1      0    ...      \n",
      "19982     0     0     1     0     1     1     0     0     0      0    ...      \n",
      "19983     1     1     0     0     1     0     1     0     1      0    ...      \n",
      "19984     0     0     0     1     0     1     0     0     0      0    ...      \n",
      "19985     0     1     0     0     0     0     0     0     1      1    ...      \n",
      "19986     0     1     0     0     0     0     0     0     1      1    ...      \n",
      "19987     0     0     0     0     1     1     1     0     0      0    ...      \n",
      "19988     0     1     0     0     1     0     0     0     1      0    ...      \n",
      "19989     0     0     0     0     0     1     0     0     0      0    ...      \n",
      "19990     0     1     0     0     0     0     0     0     1      0    ...      \n",
      "19991     1     0     0     0     0     1     0     0     0      0    ...      \n",
      "19992     0     1     0     0     0     1     0     0     1      1    ...      \n",
      "19993     0     0     0     1     0     1     0     0     0      0    ...      \n",
      "19994     1     0     0     0     1     1     0     0     0      0    ...      \n",
      "19995     0     0     0     0     1     0     0     0     0      0    ...      \n",
      "19996     0     0     0     1     0     0     0     0     0      0    ...      \n",
      "19997     1     1     0     1     0     0     0     0     1      0    ...      \n",
      "19998     1     0     0     0     1     0     0     0     0      0    ...      \n",
      "19999     1     1     0     1     1     0     1     0     1      1    ...      \n",
      "\n",
      "          cont6     cont7    cont8    cont9   cont10    cont11    cont12  \\\n",
      "0      0.718367  0.335060  0.30260  0.67135  0.83510  0.569745  0.594646   \n",
      "1      0.438917  0.436585  0.60087  0.35127  0.43919  0.338312  0.366307   \n",
      "2      0.289648  0.315545  0.27320  0.26076  0.32446  0.381398  0.373424   \n",
      "3      0.440945  0.391128  0.31796  0.32128  0.44467  0.327915  0.321570   \n",
      "4      0.178193  0.247408  0.24564  0.22089  0.21230  0.204687  0.202213   \n",
      "5      0.364464  0.401162  0.26847  0.46226  0.50556  0.366788  0.359249   \n",
      "6      0.381515  0.363768  0.24564  0.40455  0.47225  0.334828  0.352251   \n",
      "7      0.867021  0.583389  0.90267  0.84847  0.80218  0.644013  0.785706   \n",
      "8      0.628534  0.384099  0.61229  0.38249  0.51111  0.682315  0.669033   \n",
      "9      0.713343  0.469223  0.30260  0.67135  0.83510  0.863052  0.879347   \n",
      "10     0.429383  0.877905  0.39455  0.53565  0.50556  0.550529  0.538473   \n",
      "11     0.314683  0.370419  0.58354  0.46226  0.38016  0.644013  0.665644   \n",
      "12     0.408772  0.363312  0.32843  0.32128  0.44467  0.327915  0.321570   \n",
      "13     0.241574  0.255339  0.58934  0.32496  0.26029  0.257148  0.253044   \n",
      "14     0.894903  0.586433  0.80058  0.93383  0.78770  0.880469  0.871011   \n",
      "15     0.570733  0.547756  0.80438  0.44352  0.63026  0.385085  0.377003   \n",
      "16     0.411902  0.593548  0.31796  0.38846  0.48889  0.457203  0.447145   \n",
      "17     0.688705  0.437192  0.67263  0.83505  0.59334  0.678924  0.665644   \n",
      "18     0.443265  0.637086  0.36636  0.52938  0.39068  0.678924  0.665644   \n",
      "19     0.436312  0.544355  0.48864  0.36285  0.20496  0.388786  0.406090   \n",
      "20     0.441525  0.437192  0.31796  0.32128  0.44467  0.377724  0.369858   \n",
      "21     0.349885  0.381185  0.81542  0.32311  0.36458  0.453334  0.454705   \n",
      "22     0.183243  0.253560  0.40028  0.21374  0.19431  0.167024  0.165648   \n",
      "23     0.373500  0.381883  0.36083  0.44352  0.45017  0.338312  0.366307   \n",
      "24     0.382070  0.451203  0.33906  0.47900  0.54433  0.812519  0.800726   \n",
      "25     0.592478  0.496452  0.29758  0.46226  0.51111  0.434083  0.424625   \n",
      "26     0.435733  0.769905  0.60087  0.40252  0.28677  0.550529  0.538473   \n",
      "27     0.373500  0.356037  0.36083  0.44352  0.45017  0.291268  0.295524   \n",
      "28     0.671307  0.464924  0.33906  0.62542  0.66076  0.607500  0.594646   \n",
      "29     0.557431  0.402942  0.34445  0.52728  0.79139  0.377724  0.369858   \n",
      "...         ...       ...      ...      ...      ...       ...       ...   \n",
      "19970  0.211915  0.310367  0.29758  0.34365  0.30529  0.245410  0.241676   \n",
      "19971  0.220686  0.268988  0.27797  0.38249  0.31003  0.245410  0.241676   \n",
      "19972  0.704364  0.562866  0.34987  0.44767  0.53881  0.492200  0.481306   \n",
      "19973  0.460158  0.440595  0.29758  0.50420  0.54983  0.453334  0.462286   \n",
      "19974  0.385965  0.369498  0.36083  0.46853  0.52221  0.377724  0.369858   \n",
      "19975  0.460158  0.462518  0.29758  0.50420  0.54983  0.453334  0.462286   \n",
      "19976  0.321828  0.312802  0.26847  0.28922  0.52775  0.415029  0.406090   \n",
      "19977  0.244387  0.242606  0.28280  0.32311  0.23148  0.245410  0.241676   \n",
      "19978  0.375429  0.389249  0.41182  0.42289  0.45017  0.341813  0.335036   \n",
      "19979  0.699195  0.726866  0.30768  0.38249  0.69471  0.678924  0.665644   \n",
      "19980  0.456654  0.457075  0.36083  0.46853  0.51666  0.581178  0.568648   \n",
      "19981  0.508584  0.675139  0.33372  0.57506  0.54433  0.927095  0.920142   \n",
      "19982  0.605753  0.388663  0.28768  0.39046  0.53881  0.614915  0.601984   \n",
      "19983  0.220282  0.303334  0.27797  0.38249  0.31003  0.245410  0.241676   \n",
      "19984  0.938223  0.583629  0.58934  0.97621  0.83510  0.511698  0.630853   \n",
      "19985  0.875461  0.589016  0.96202  0.86299  0.83510  0.739860  0.772574   \n",
      "19986  0.389874  0.398557  0.53642  0.42700  0.52221  0.291268  0.286079   \n",
      "19987  0.287716  0.316206  0.24564  0.38249  0.31480  0.301022  0.305148   \n",
      "19988  0.187505  0.322751  0.29758  0.32128  0.21230  0.209813  0.207184   \n",
      "19989  0.377914  0.352312  0.26847  0.38846  0.43919  0.257148  0.253044   \n",
      "19990  0.581931  0.907928  0.32317  0.46226  0.50556  0.797841  0.785706   \n",
      "19991  0.858100  0.681493  0.82252  0.58325  0.80569  0.832976  0.832658   \n",
      "19992  0.287234  0.337925  0.50658  0.33611  0.31480  0.278556  0.289207   \n",
      "19993  0.889243  0.839287  0.80438  0.81945  0.80218  0.797841  0.862189   \n",
      "19994  0.631275  0.439865  0.63475  0.79734  0.64056  0.415029  0.406090   \n",
      "19995  0.416753  0.513105  0.54236  0.39849  0.45017  0.422624  0.481306   \n",
      "19996  0.434866  0.701285  0.68308  0.34365  0.43919  0.763153  0.750403   \n",
      "19997  0.493591  0.665542  0.29758  0.55233  0.53328  0.577376  0.564899   \n",
      "19998  0.317480  0.388663  0.71817  0.36285  0.39599  0.341813  0.352251   \n",
      "19999  0.924754  0.847563  0.31796  0.81945  0.80914  0.821840  0.810292   \n",
      "\n",
      "         cont13    cont14      loss  \n",
      "0      0.822493  0.714843  7.702186  \n",
      "1      0.611431  0.304496  7.157424  \n",
      "2      0.195709  0.774425  8.008063  \n",
      "3      0.605077  0.602642  6.845720  \n",
      "4      0.246011  0.432606  7.924380  \n",
      "5      0.345247  0.726792  8.545367  \n",
      "6      0.342239  0.382931  7.031936  \n",
      "7      0.859764  0.242416  8.184723  \n",
      "8      0.756454  0.361191  9.237975  \n",
      "9      0.822493  0.294523  8.729816  \n",
      "10     0.336261  0.715009  8.763561  \n",
      "11     0.339244  0.799124  8.693787  \n",
      "12     0.605077  0.818358  7.084268  \n",
      "13     0.276878  0.477578  6.977067  \n",
      "14     0.822493  0.251278  6.371919  \n",
      "15     0.516660  0.340325  7.240972  \n",
      "16     0.301535  0.205651  8.796236  \n",
      "17     0.684242  0.407411  7.885593  \n",
      "18     0.304350  0.310796  8.335028  \n",
      "19     0.648701  0.830931  8.242201  \n",
      "20     0.605077  0.743810  7.052271  \n",
      "21     0.651733  0.354002  6.792502  \n",
      "22     0.404520  0.725941  6.641143  \n",
      "23     0.339244  0.793518  6.648440  \n",
      "24     0.246011  0.215055  8.889652  \n",
      "25     0.357400  0.311644  7.332193  \n",
      "26     0.298734  0.698006  8.473674  \n",
      "27     0.339244  0.804795  7.679700  \n",
      "28     0.678452  0.285224  9.365036  \n",
      "29     0.687115  0.297788  7.469369  \n",
      "...         ...       ...       ...  \n",
      "19970  0.222124  0.297178  7.329310  \n",
      "19971  0.298734  0.231719  8.444526  \n",
      "19972  0.654753  0.236662  8.474438  \n",
      "19973  0.312885  0.469526  7.165470  \n",
      "19974  0.321548  0.219861  7.107696  \n",
      "19975  0.312885  0.602619  7.424708  \n",
      "19976  0.213045  0.264067  7.755613  \n",
      "19977  0.324464  0.258291  8.186122  \n",
      "19978  0.382252  0.233688  7.714517  \n",
      "19979  0.684242  0.211318  7.650740  \n",
      "19980  0.363547  0.837845  6.737750  \n",
      "19981  0.351299  0.602176  8.755159  \n",
      "19982  0.666708  0.708133  9.302457  \n",
      "19983  0.298734  0.379909  8.150976  \n",
      "19984  0.915805  0.840615  7.083899  \n",
      "19985  0.808455  0.253095  7.662407  \n",
      "19986  0.287682  0.233130  8.754162  \n",
      "19987  0.276878  0.376271  9.091171  \n",
      "19988  0.261150  0.318657  7.166806  \n",
      "19989  0.608259  0.239368  7.322656  \n",
      "19990  0.315758  0.377216  8.062786  \n",
      "19991  0.866072  0.730210  6.777692  \n",
      "19992  0.333292  0.602619  8.643586  \n",
      "19993  0.846403  0.301640  9.361891  \n",
      "19994  0.738850  0.804571  7.131730  \n",
      "19995  0.333292  0.427217  7.510414  \n",
      "19996  0.195709  0.276850  9.584224  \n",
      "19997  0.287682  0.837303  6.454538  \n",
      "19998  0.266328  0.216911  7.578263  \n",
      "19999  0.864518  0.285079  9.515829  \n",
      "\n",
      "[20000 rows x 131 columns]\n"
     ]
    }
   ],
   "source": [
    "cat_feature = [n for n in train_data.columns if n.startswith('cat')]    \n",
    "cont_feature = [n for n in train_data.columns if n.startswith('cont')] \n",
    "\n",
    "for column in cat_feature:\n",
    "        train_data[column] = pandas.factorize(train_data[column].values, sort=True)[0]\n",
    "        \n",
    "print train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        7.702186\n",
      "1        7.157424\n",
      "2        8.008063\n",
      "3        6.845720\n",
      "4        7.924380\n",
      "5        8.545367\n",
      "6        7.031936\n",
      "7        8.184723\n",
      "8        9.237975\n",
      "9        8.729816\n",
      "10       8.763561\n",
      "11       8.693787\n",
      "12       7.084268\n",
      "13       6.977067\n",
      "14       6.371919\n",
      "15       7.240972\n",
      "16       8.796236\n",
      "17       7.885593\n",
      "18       8.335028\n",
      "19       8.242201\n",
      "20       7.052271\n",
      "21       6.792502\n",
      "22       6.641143\n",
      "23       6.648440\n",
      "24       8.889652\n",
      "25       7.332193\n",
      "26       8.473674\n",
      "27       7.679700\n",
      "28       9.365036\n",
      "29       7.469369\n",
      "           ...   \n",
      "19970    7.329310\n",
      "19971    8.444526\n",
      "19972    8.474438\n",
      "19973    7.165470\n",
      "19974    7.107696\n",
      "19975    7.424708\n",
      "19976    7.755613\n",
      "19977    8.186122\n",
      "19978    7.714517\n",
      "19979    7.650740\n",
      "19980    6.737750\n",
      "19981    8.755159\n",
      "19982    9.302457\n",
      "19983    8.150976\n",
      "19984    7.083899\n",
      "19985    7.662407\n",
      "19986    8.754162\n",
      "19987    9.091171\n",
      "19988    7.166806\n",
      "19989    7.322656\n",
      "19990    8.062786\n",
      "19991    6.777692\n",
      "19992    8.643586\n",
      "19993    9.361891\n",
      "19994    7.131730\n",
      "19995    7.510414\n",
      "19996    9.584224\n",
      "19997    6.454538\n",
      "19998    7.578263\n",
      "19999    9.515829\n",
      "Name: loss, Length: 20000, dtype: float64\n",
      "(18000,)\n",
      "(18000, 130)\n"
     ]
    }
   ],
   "source": [
    "#get the number of rows and columns\n",
    "c =train_data.shape[1]\n",
    "#create an array which has indexes of columns\n",
    "i_cols = []\n",
    "for i in range(0,c-1):\n",
    "    i_cols.append(i)\n",
    "\n",
    "#Y is the target column, X has the rest\n",
    "X = train_data.iloc[:X_train_end,0:(c-1)]\n",
    "Y = train_data.iloc[:X_train_end,(c-1)]\n",
    "#Y = Y.reshape(-1, 1)\n",
    "\n",
    "#del dataset_encoded\n",
    "#Validation chunk size\n",
    "val_size = 0.1\n",
    "\n",
    "#Use a common seed in all experiments so that same chunk is used for validation\n",
    "seed = 0\n",
    "#Split the data into chunks\n",
    "from sklearn import cross_validation\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = cross_validation.train_test_split(X, Y, test_size=val_size, random_state=seed)\n",
    "\n",
    "\n",
    "del X\n",
    "del Y\n",
    "#print (Y_train.shape)\n",
    "#print (X_train.shape)\n",
    "#All features\n",
    "X_all = []\n",
    "\n",
    "#List of combinations\n",
    "comb = []\n",
    "\n",
    "#Dictionary to store the MAE for all algorithms \n",
    "mae = []\n",
    "\n",
    "#Scoring parameter\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "n = \"All\"\n",
    "#X_all.append([n, X_train,X_val,i_cols])\n",
    "X_all.append([n, i_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1245.07483494\n"
     ]
    }
   ],
   "source": [
    "#X_train.isnull().any()\n",
    "#print X_train\n",
    "#print np.any(np.isnan(X_train))\n",
    "#Evaluation of various combinations of LinearRegression\n",
    "#Import the library\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#uncomment the below lines if you want to run the algo\n",
    "##Set the base model\n",
    "model = LinearRegression(n_jobs=-1)\n",
    "#algo = \"LR\"\n",
    "#Y_test = Y_test.reshape(-1, 1)\n",
    "model.fit(X_train,Y_train)\n",
    "'''\n",
    "for i in range(len(X_test)):\n",
    "    if np.isinf(np.exp(model.predict(X_test[i]))) == True:\n",
    "        print (i ,\" \",X_test[i],' ' ,model.predict(X_test[i]),\" \",Y_test[i])\n",
    "        X_test[i] = 0\n",
    "        Y_test[i] = 0\n",
    "        print (i ,\" \", model.predict(X_test[i]),\" \",Y_test[i])\n",
    "        \n",
    "#- np.exp(Y_test)\n",
    "'''\n",
    "result = mean_absolute_error(np.exp(model.predict(X_test)),np.exp(Y_test))\n",
    "print(result)\n",
    "\n",
    "\n",
    "##Plot the MAE of all combinations\n",
    "#fig, ax = plt.subplots()\n",
    "#plt.plot(mae)\n",
    "##Set the tick names to names of combinations\n",
    "#ax.set_xticks(range(len(comb)))\n",
    "#ax.set_xticklabels(comb,rotation='vertical')\n",
    "##Plot the accuracy for all combinations\n",
    "#plt.show()    \n",
    "\n",
    "#MAE achieved is 1278\n",
    "#mine was on 20000 to get 1245.07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1245.07483494\n"
     ]
    }
   ],
   "source": [
    "print result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = train_data.iloc[X_test_start:,0:(c-1)]\n",
    "prediction = np.exp(model.predict(X))\n",
    "print prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame()\n",
    "submission['id'] = ids\n",
    "submission['loss'] = prediction\n",
    "submission.to_csv('sub_v.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
