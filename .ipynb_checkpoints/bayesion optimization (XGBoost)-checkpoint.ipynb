{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing the data again "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do the prepprocessing of the data as done on the previous part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Supress unnecessary warnings so that presentation looks clean\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas                                      #to read and manipulate data\n",
    "import zipfile                                     #to extract data\n",
    "import numpy as np                                 #for matrix operations\n",
    "#read will be imported as and when required\n",
    "#read the train and test zip file\n",
    "zip_ref = zipfile.ZipFile(\"train.csv.zip\", 'r')    \n",
    "zip_ref.extractall()                               \n",
    "zip_ref.close()\n",
    "\n",
    "train_data = pandas.read_csv(\"train.csv\")\n",
    "\n",
    "import copy\n",
    "test_data = copy.deepcopy(train_data.iloc[150000:])\n",
    "train_data = train_data.iloc[:150000]\n",
    "\n",
    "y_true = test_data['loss']\n",
    "\n",
    "ids = test_data['id']\n",
    "\n",
    "target = train_data['loss']\n",
    "\n",
    "#drop the unnecessary column id and loss from both train and test set.\n",
    "train_data.drop(['id','loss'],1,inplace=True)\n",
    "test_data.drop(['id','loss'],1,inplace=True)\n",
    "\n",
    "shift = 200\n",
    "target = np.log(target+shift)\n",
    "\n",
    "#merging both the datasets to make single joined dataset\n",
    "joined = pandas.concat([train_data, test_data],ignore_index = True)\n",
    "del train_data,test_data                                         #deleting previous one to save memory.\n",
    "\n",
    "cat_feature = [n for n in joined.columns if n.startswith('cat')]  #list of all the features containing categorical values\n",
    "\n",
    "#factorizing them\n",
    "for column in cat_feature:\n",
    "    joined[column] = pandas.factorize(joined[column].values, sort=True)[0]\n",
    "        \n",
    "del cat_feature\n",
    "\n",
    "#dividing the training data between training and testing set\n",
    "train_data = joined.iloc[:150000,:]\n",
    "test_data = joined.iloc[150000:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/demonicode/.local/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#inporting additional files\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from bayes_opt import BayesianOptimization\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#making the function to be used for different values of hyper-parameter\n",
    "def xgb_evaluate(min_child_weight,colsample_bytree,max_depth,subsample,gamma,alpha):\n",
    "    params['min_child_weight'] = int(min_child_weight)\n",
    "    params['cosample_bytree'] = max(min(colsample_bytree, 1), 0)\n",
    "    params['max_depth'] = int(max_depth)\n",
    "    params['subsample'] = max(min(subsample, 1), 0)\n",
    "    params['gamma'] = max(gamma, 0)\n",
    "    params['alpha'] = max(alpha, 0)\n",
    "\n",
    "    cv_result = xgb.cv(params, xgtrain, num_boost_round=num_rounds, nfold=5,seed=random_state,\n",
    "             callbacks=[xgb.callback.early_stop(50)])\n",
    "    \n",
    "    #returning negative of cv result since, xgb.cv maximizes the score and we need to minimize the error\n",
    "    return -cv_result['test-mae-mean'].values[-1]              \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#making xgb matrix to be used as input\n",
    "xgtrain = xgb.DMatrix(train_data, label=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n#although after running a couple of iterations , it was seen that num_rounds never crossed even 1000, but \\n#to not take a risk, we set it to 3000\\nnum_rounds = 3000\\n#random seed value to make the results replicable\\nrandom_state = 2016\\nnum_iter = 25\\ninit_points = 5\\nparams = {'eta': 0.1,'silent': 1,'eval_metric': 'mae','verbose_eval': 2,'seed': random_state}\\n\\nxgbBO = BayesianOptimization(xgb_evaluate, {'min_child_weight': (1, 20),\\n                                                'colsample_bytree': (0.1, 1),\\n                                                'max_depth': (5, 15),\\n                                                'subsample': (0.5, 1),\\n                                                'gamma': (0, 10),\\n                                                'alpha': (0, 10),\\n                                                })\\n\\n#using bayesian optimization to maximize the passed score like accuracy\\nxgbBO.maximize(init_points=init_points, n_iter=num_iter)\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#although after running a couple of iterations , it was seen that num_rounds never crossed even 1000, but \n",
    "#to not take a risk, we set it to 3000\n",
    "num_rounds = 3000\n",
    "#random seed value to make the results replicable\n",
    "random_state = 2016\n",
    "num_iter = 25\n",
    "init_points = 5\n",
    "params = {'eta': 0.1,'silent': 1,'eval_metric': 'mae','verbose_eval': 2,'seed': random_state}\n",
    "\n",
    "xgbBO = BayesianOptimization(xgb_evaluate, {'min_child_weight': (1, 20),\n",
    "                                                'colsample_bytree': (0.1, 1),\n",
    "                                                'max_depth': (5, 15),\n",
    "                                                'subsample': (0.5, 1),\n",
    "                                                'gamma': (0, 10),\n",
    "                                                'alpha': (0, 10),\n",
    "                                                })\n",
    "\n",
    "#using bayesian optimization to maximize the passed score like accuracy\n",
    "xgbBO.maximize(init_points=init_points, n_iter=num_iter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, it is a computationally heavy task to run model so many times, i couldn't do it on my laptop even after running for hours, therefore, i used google cloud to run this code as a script. Even on a 8 core 50 gb ram machine, it took x hours to complete, I'm posting the results below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE- this code is taken from Vladimir Iglovikov github repository. Link - https://github.com/fmfn/BayesianOptimization/blob/master/examples/xgboost_example.py\n",
    "\n",
    "Minor modifications were also tried but this provides the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
