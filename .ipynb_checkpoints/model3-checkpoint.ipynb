{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import copy\n",
    "import zipfile\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import cross_validation, metrics   \n",
    "from sklearn.grid_search import GridSearchCV\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 4\n",
    "\n",
    "\n",
    "zip_ref = zipfile.ZipFile(\"train.csv.zip\", 'r')\n",
    "zip_ref.extractall()\n",
    "zip_ref.close()\n",
    "zip_ref2 = zipfile.ZipFile(\"test.csv.zip\", 'r')\n",
    "zip_ref2.extractall()\n",
    "zip_ref2.close()\n",
    "\n",
    "\n",
    "train_data = pandas.read_csv(\"train.csv\")\n",
    "test_data = pandas.read_csv(\"test.csv\")\n",
    "test_data['loss'] = np.nan\n",
    "\n",
    "\n",
    "joined = pandas.concat([train_data, test_data],ignore_index = True)\n",
    "del train_data,test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_feature = [n for n in joined.columns if n.startswith('cat')]    \n",
    "#cont_feature = [n for n in joined.columns if n.startswith('cont')] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for column in cat_feature:\n",
    "        joined[column] = pandas.factorize(joined[column].values, sort=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = joined[joined['loss'].notnull()]\n",
    "del cat_feature\n",
    "\n",
    "\n",
    "\n",
    "train_data = joined[joined['loss'].notnull()]\n",
    "test_data = joined[joined['loss'].isnull()]\n",
    "del joined\n",
    "\n",
    "ids = test_data['id']\n",
    "\n",
    "\n",
    "shift = 700\n",
    "train_data[\"loss\"] = np.log(train_data[\"loss\"]+shift)\n",
    "target = train_data['loss']\n",
    "train_data.drop(['id','loss'],1,inplace=True)\n",
    "test_data.drop(['id','loss'],1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef modelfit(alg, dtrain,useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\\n    \\n    if useTrainCV:\\n        xgb_param = alg.get_xgb_params()\\n        xgtrain = xgb.DMatrix(train_data, label=target)\\n        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()[\\'n_estimators\\'], nfold=cv_folds,            feval=evalerror, early_stopping_rounds=early_stopping_rounds)\\n        print (cvresult)\\n        alg.set_params(n_estimators=cvresult.shape[0])\\n        \\n    \\n    #Fit the algorithm on the data\\n    return alg.fit(train_data, target,eval_metric=evalerror)\\n       \\n    #Predict training set:\\n    dtrain_predictions = alg.predict(dtrain[predictors])\\n    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\\n        \\n    #Print model report:\\n    print (\"\\nModel Report\")\\n    print (\"Accuracy : %.4g\" % metrics.accuracy_score(dtrain[\\'loss\\'].values, dtrain_predictions))\\n    print (\"mae Score (Train): %f\" % metrics.roc_auc_score(dtrain[\\'loss\\'], dtrain_predprob))\\n                    \\n    feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\\n    feat_imp.plot(kind=\\'bar\\', title=\\'Feature Importances\\')\\n    plt.ylabel(\\'Feature Importance Score\\')\\n    '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#train_data = train.iloc[:,1:train.shape[1]-1]\n",
    "#ids = test['id']\n",
    "#X = train_data.drop(['loss', 'id'], 1)\n",
    "'''\n",
    "params = {}\n",
    "params['booster'] = 'gbtree'\n",
    "params['objective'] = \"reg:linear\"\n",
    "params['eval_metric'] = 'mae'\n",
    "params['eta'] = 0.1\n",
    "params['gamma'] = 0.5290\n",
    "params['min_child_weight'] = 4.2922\n",
    "params['colsample_bytree'] = 0.3085\n",
    "params['subsample'] = 0.9930\n",
    "params['max_depth'] = 7\n",
    "params['max_delta_step'] = 0\n",
    "params['silent'] = 1\n",
    "\n",
    "d_train_full = xgb.DMatrix(train_data, label=train_data[loss])\n",
    "d_test = xgb.DMatrix(test_data)\n",
    "params['random_state'] = RANDOM_STATE\n",
    "'''\n",
    "def evalerror(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'mae', mean_absolute_error(np.exp(preds), np.exp(labels))\n",
    "'''\n",
    "def modelfit(alg, dtrain,useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n",
    "    \n",
    "    if useTrainCV:\n",
    "        xgb_param = alg.get_xgb_params()\n",
    "        xgtrain = xgb.DMatrix(train_data, label=target)\n",
    "        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\\\n",
    "            feval=evalerror, early_stopping_rounds=early_stopping_rounds)\n",
    "        print (cvresult)\n",
    "        alg.set_params(n_estimators=cvresult.shape[0])\n",
    "        \n",
    "    \n",
    "    #Fit the algorithm on the data\n",
    "    return alg.fit(train_data, target,eval_metric=evalerror)\n",
    "       \n",
    "    #Predict training set:\n",
    "    dtrain_predictions = alg.predict(dtrain[predictors])\n",
    "    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n",
    "        \n",
    "    #Print model report:\n",
    "    print (\"\\nModel Report\")\n",
    "    print (\"Accuracy : %.4g\" % metrics.accuracy_score(dtrain['loss'].values, dtrain_predictions))\n",
    "    print (\"mae Score (Train): %f\" % metrics.roc_auc_score(dtrain['loss'], dtrain_predprob))\n",
    "                    \n",
    "    feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n",
    "    feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "    plt.ylabel('Feature Importance Score')\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-146386be8089>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0md_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mcvresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_train_full\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_boost_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnfold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m            \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevalerror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mcvresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/demonicode/.local/lib/python2.7/site-packages/xgboost/training.pyc\u001b[0m in \u001b[0;36mcv\u001b[0;34m(params, dtrain, num_boost_round, nfold, stratified, folds, metrics, obj, feval, maximize, early_stopping_rounds, fpreproc, as_pandas, verbose_eval, show_stdv, seed, callbacks)\u001b[0m\n\u001b[1;32m    398\u001b[0m                            evaluation_result_list=None))\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcvfolds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mfold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maggcv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcvfolds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/demonicode/.local/lib/python2.7/site-packages/xgboost/training.pyc\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, iteration, fobj)\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;34m\"\"\"\"Update the boosters for one iteration\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/demonicode/.local/lib/python2.7/site-packages/xgboost/core.pyc\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m             \u001b[0m_check_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBoosterUpdateOneIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "params = {\n",
    "        'min_child_weight': 1,\n",
    "        'eta': 0.01,\n",
    "        'colsample_bytree': 0.5,\n",
    "        'max_depth': 12,\n",
    "        'subsample': 0.8,\n",
    "        'alpha': 1,\n",
    "        'gamma': 1,\n",
    "        'silent': 1,\n",
    "        'verbose_eval': True,\n",
    "        'seed': 2016\n",
    "    }\n",
    "\n",
    "d_train_full = xgb.DMatrix(train_data, label=target)\n",
    "d_test = xgb.DMatrix(test_data)\n",
    "\n",
    "cvresult = xgb.cv(params, d_train_full, num_boost_round=1000, nfold=5,\\\n",
    "            feval=evalerror, early_stopping_rounds=50)\n",
    "print cvresult\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   test-mae-mean  test-mae-std  train-mae-mean  train-mae-std\n",
      "0    3733.832275     19.836743     3733.832129       4.959070\n",
      "1    3730.431397     19.836997     3730.431738       4.957903\n",
      "2    3724.608740     19.838311     3724.608740       4.954066\n",
      "3    3715.244531     19.838042     3715.244287       4.948037\n",
      "4    3701.022070     19.838690     3701.022558       4.937156\n",
      "5    3680.508545     19.830986     3680.508789       4.927406\n",
      "6    3652.260303     19.819283     3652.256250       4.913685\n",
      "7    3614.943018     19.793982     3614.935644       4.901381\n",
      "8    3567.470557     19.733455     3567.461475       4.893843\n",
      "9    3509.112354     19.705653     3509.098242       4.833370\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nparams = {\\n        'min_child_weight': 1,\\n        'eta': 0.01,\\n        'colsample_bytree': 0.5,\\n        'max_depth': 12,\\n        'subsample': 0.8,\\n        'alpha': 1,\\n        'gamma': 1,\\n        'silent': 1,\\n        'verbose_eval': True,\\n        'seed': 2016\\n    }\\n    \\nd_train_full = xgb.DMatrix(train_data, label=target)\\nd_test = xgb.DMatrix(test_data)\\n\\ncvresult = xgb.cv(params, d_train_full, num_boost_round=1000, nfold=5,            feval=evalerror, early_stopping_rounds=50)\\nprint cvresult\\n#alg.set_params(n_estimators=cvresult.shape[0])\\n    \\n#Fit the algorithm on the data\\n#return alg.fit(train_data, target,eval_metric=evalerror)\\n\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb1 = XGBClassifier(n_estimators=2000,\n",
    " learning_rate =0.1,\n",
    " max_depth=5,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'reg:linear',\n",
    " scale_pos_weight=1,\n",
    " seed=2016)\n",
    "modelfit(xgb1, train_data)\n",
    "\n",
    "'''\n",
    "params = {\n",
    "        'min_child_weight': 1,\n",
    "        'eta': 0.01,\n",
    "        'colsample_bytree': 0.5,\n",
    "        'max_depth': 12,\n",
    "        'subsample': 0.8,\n",
    "        'alpha': 1,\n",
    "        'gamma': 1,\n",
    "        'silent': 1,\n",
    "        'verbose_eval': True,\n",
    "        'seed': 2016\n",
    "    }\n",
    "    \n",
    "d_train_full = xgb.DMatrix(train_data, label=target)\n",
    "d_test = xgb.DMatrix(test_data)\n",
    "\n",
    "cvresult = xgb.cv(params, d_train_full, num_boost_round=1000, nfold=5,\\\n",
    "            feval=evalerror, early_stopping_rounds=50)\n",
    "print cvresult\n",
    "#alg.set_params(n_estimators=cvresult.shape[0])\n",
    "'''    \n",
    "#Fit the algorithm on the data\n",
    "#return alg.fit(train_data, target,eval_metric=evalerror)\n",
    "\n",
    "'''\n",
    "     test-mae-mean  test-mae-std  test-rmse-mean  test-rmse-std  \\\n",
    "0      3733.832275     19.836743        6.795677       0.004212   \n",
    "1      3730.431397     19.836997        6.119591       0.004220   \n",
    "2      3724.608740     19.838311        5.511544       0.004306   \n",
    "3      3715.244531     19.838042        4.964556       0.004250   \n",
    "4      3701.022070     19.838690        4.472667       0.004241   \n",
    "5      3680.508545     19.830986        4.030425       0.004140   \n",
    "6      3652.260303     19.819283        3.632943       0.004120   \n",
    "7      3614.943115     19.793942        3.275650       0.004121   \n",
    "8      3567.470557     19.733455        2.954614       0.004052   \n",
    "9      3509.112354     19.705653        2.666321       0.004032   \n",
    "10     3439.547901     19.665581        2.407569       0.004037   \n",
    "11     3358.897266     19.622429        2.175405       0.004045   \n",
    "12     3267.701416     19.582996        1.967359       0.004060   \n",
    "13     3166.962354     19.527927        1.780991       0.004057   \n",
    "14     3057.882275     19.536230        1.614288       0.004106   \n",
    "15     2941.996631     19.538504        1.465280       0.004146   \n",
    "16     2821.029492     19.439302        1.332279       0.004072   \n",
    "17     2696.572168     19.319719        1.213766       0.004055   \n",
    "18     2570.829590     19.293713        1.108427       0.004037   \n",
    "19     2445.221289     19.220234        1.015083       0.003981   \n",
    "20     2321.919140     18.924452        0.932467       0.003836   \n",
    "21     2202.752930     18.905659        0.859568       0.003816   \n",
    "22     2089.275195     18.701260        0.795437       0.003807   \n",
    "23     1982.758130     18.514150        0.739339       0.003750   \n",
    "24     1884.732153     18.118085        0.690487       0.003696   \n",
    "25     1795.759668     17.794437        0.648107       0.003607   \n",
    "26     1716.039355     17.385725        0.611386       0.003584   \n",
    "27     1645.678125     16.741626        0.579754       0.003457   \n",
    "28     1584.119653     16.267360        0.552754       0.003243   \n",
    "29     1530.430493     15.528565        0.529716       0.003051   \n",
    "..             ...           ...             ...            ...   \n",
    "798    1144.146338      6.963795        0.390488       0.001075   \n",
    "799    1144.142603      6.980179        0.390485       0.001077   \n",
    "800    1144.131885      6.971224        0.390488       0.001072   \n",
    "801    1144.118603      6.993638        0.390486       0.001077   \n",
    "802    1144.109448      7.003246        0.390485       0.001078   \n",
    "803    1144.106714      7.021788        0.390480       0.001077   \n",
    "804    1144.103467      7.015384        0.390477       0.001075   \n",
    "805    1144.110205      6.987450        0.390474       0.001065   \n",
    "806    1144.103345      6.974451        0.390470       0.001060   \n",
    "807    1144.100073      6.995141        0.390464       0.001063   \n",
    "808    1144.090576      6.995297        0.390463       0.001062   \n",
    "809    1144.085718      6.966939        0.390464       0.001062   \n",
    "810    1144.086914      6.958481        0.390463       0.001060   \n",
    "811    1144.071826      6.960364        0.390460       0.001061   \n",
    "812    1144.077954      6.948537        0.390462       0.001061   \n",
    "813    1144.084278      6.987706        0.390464       0.001064   \n",
    "814    1144.099634      6.943866        0.390465       0.001062   \n",
    "815    1144.098071      6.948638        0.390465       0.001061   \n",
    "816    1144.109766      6.950613        0.390459       0.001062   \n",
    "817    1144.105176      6.953750        0.390456       0.001067   \n",
    "818    1144.124756      6.940586        0.390458       0.001071   \n",
    "819    1144.120410      6.950960        0.390460       0.001071   \n",
    "820    1144.134717      6.937818        0.390462       0.001068   \n",
    "821    1144.157251      6.925102        0.390465       0.001064   \n",
    "822    1144.163940      6.924035        0.390466       0.001061   \n",
    "823    1144.174487      6.934519        0.390463       0.001069   \n",
    "824    1144.188892      6.950216        0.390462       0.001069   \n",
    "825    1144.179858      6.929558        0.390459       0.001065   \n",
    "826    1144.190405      6.925802        0.390455       0.001061   \n",
    "827    1144.193188      6.928327        0.390454       0.001063   \n",
    "\n",
    "     train-mae-mean  train-mae-std  train-rmse-mean  train-rmse-std  \n",
    "0       3733.832129       4.959070         6.795701        0.000950  \n",
    "1       3730.431738       4.957903         6.119623        0.000881  \n",
    "2       3724.608740       4.954066         5.511524        0.000720  \n",
    "3       3715.244287       4.948037         4.964536        0.000658  \n",
    "4       3701.022558       4.937156         4.472651        0.000606  \n",
    "5       3680.508789       4.927406         4.030392        0.000625  \n",
    "6       3652.256152       4.913550         3.632854        0.000598  \n",
    "7       3614.935644       4.901381         3.275546        0.000552  \n",
    "8       3567.461475       4.893843         2.954496        0.000504  \n",
    "9       3509.098242       4.833370         2.666176        0.000448  \n",
    "10      3439.521094       4.791028         2.407376        0.000451  \n",
    "11      3358.867090       4.722829         2.175206        0.000420  \n",
    "12      3267.652197       4.628512         1.967108        0.000378  \n",
    "13      3166.902246       4.532571         1.780711        0.000394  \n",
    "14      3057.782910       4.338821         1.613956        0.000340  \n",
    "15      2941.868799       4.171512         1.464896        0.000297  \n",
    "16      2820.802197       4.063352         1.331794        0.000344  \n",
    "17      2696.256006       3.951326         1.213205        0.000325  \n",
    "18      2570.434180       3.783835         1.107837        0.000298  \n",
    "19      2444.646436       3.698985         1.014391        0.000325  \n",
    "20      2321.122803       3.685049         0.931635        0.000320  \n",
    "21      2201.728271       3.380780         0.858662        0.000368  \n",
    "22      2088.003369       3.131028         0.794449        0.000337  \n",
    "23      1981.169067       2.849189         0.738256        0.000366  \n",
    "24      1882.845849       2.633151         0.689275        0.000369  \n",
    "25      1793.502539       2.386024         0.646767        0.000399  \n",
    "26      1713.332275       2.349155         0.609925        0.000332  \n",
    "27      1642.591260       2.321989         0.578185        0.000284  \n",
    "28      1580.460693       2.345256         0.551053        0.000426  \n",
    "29      1526.356323       2.407552         0.527916        0.000484  \n",
    "..              ...            ...              ...             ...  \n",
    "798     1037.453931       1.379518         0.358172        0.000275  \n",
    "799     1037.347290       1.368520         0.358139        0.000273  \n",
    "800     1037.271387       1.369297         0.358116        0.000272  \n",
    "801     1037.165771       1.380840         0.358082        0.000273  \n",
    "802     1037.035498       1.356376         0.358048        0.000265  \n",
    "803     1036.946777       1.361642         0.358017        0.000270  \n",
    "804     1036.863550       1.362418         0.357989        0.000269  \n",
    "805     1036.716309       1.334167         0.357951        0.000268  \n",
    "806     1036.620117       1.350745         0.357921        0.000271  \n",
    "807     1036.488086       1.343060         0.357885        0.000272  \n",
    "808     1036.389111       1.336583         0.357857        0.000270  \n",
    "809     1036.276904       1.319739         0.357831        0.000267  \n",
    "810     1036.169531       1.298306         0.357799        0.000261  \n",
    "811     1036.057251       1.285150         0.357764        0.000256  \n",
    "812     1035.966724       1.289793         0.357738        0.000259  \n",
    "813     1035.871216       1.298512         0.357705        0.000263  \n",
    "814     1035.761377       1.280766         0.357677        0.000262  \n",
    "815     1035.680346       1.271671         0.357650        0.000259  \n",
    "816     1035.589062       1.300246         0.357615        0.000256  \n",
    "817     1035.486157       1.300561         0.357579        0.000249  \n",
    "818     1035.393945       1.294971         0.357547        0.000249  \n",
    "819     1035.285815       1.290467         0.357518        0.000253  \n",
    "820     1035.205005       1.293038         0.357491        0.000250  \n",
    "821     1035.103516       1.288323         0.357457        0.000248  \n",
    "822     1035.005176       1.312836         0.357432        0.000242  \n",
    "823     1034.913355       1.312729         0.357406        0.000240  \n",
    "824     1034.829443       1.300857         0.357375        0.000243  \n",
    "825     1034.740259       1.308390         0.357345        0.000248  \n",
    "826     1034.649902       1.311145         0.357319        0.000253  \n",
    "827     1034.549267       1.325170         0.357288        0.000258  \n",
    "\n",
    "[828 rows x 8 columns]\n",
    "'''\n",
    "tempp=5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb1 = XGBClassifier(n_estimators=10,\n",
    " learning_rate =0.1,\n",
    " max_depth=5,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " objective= 'reg:linear',\n",
    " scale_pos_weight=1,\n",
    " seed=2016)\n",
    "#xgb1.set_params(n_estimators=828)\n",
    "#xgb1.set_params(n_estimators=10)\n",
    "\n",
    "#model = xgb.train(params, xgtrain, int(2012 / 0.9), feval=evalerror)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.train(params, xgtrain, int(2012 / 0.9), feval=evalerror)\n",
    "#clf = xgb1.fit(train_data[:100],target[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "scorer = make_scorer(evalerror)\n",
    "\n",
    "param_test1 = {\n",
    " 'max_depth':[12,13],\n",
    " 'min_child_weight':[1,2]\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = xgb1 , param_grid =param_test1, scoring=scorer)\n",
    "gsearch1.fit(train_data,target)\n",
    "\n",
    "print (gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_)\n",
    "'''\n",
    "param_test1 = {\n",
    " 'max_depth':[12],\n",
    " 'min_child_weight':[1]\n",
    "}\n",
    "gsearch1 = GridSearchCV(estimator = xgb1,param_grid = param_test1, scoring=evalerror,n_jobs=-1,iid=False, cv=5)\n",
    "gsearch1.fit(train_data,target)\n",
    "gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-39fb99abf9f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0md_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mshift\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msubmission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "#d_train_full = xgb.DMatrix(train_data, label=train_data[loss])\n",
    "\n",
    "d_test = xgb.DMatrix(test_data)\n",
    "\n",
    "prediction = np.exp(model.predict(d_test)) - shift\n",
    "\n",
    "submission = pandas.DataFrame()\n",
    "submission['loss'] = prediction\n",
    "submission['id'] = ids\n",
    "submission.to_csv('sub_v2.0.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_train_full = xgb.DMatrix(train_data, label=train_data[loss])\n",
    "d_test = xgb.DMatrix(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mae:5.4558\teval-mae:5.4558\n",
      "Multiple eval metrics have been passed: 'eval-mae' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mae hasn't improved in 25 rounds.\n",
      "[1]\ttrain-mae:4.91022\teval-mae:4.91022\n",
      "[2]\ttrain-mae:4.41921\teval-mae:4.41921\n",
      "[3]\ttrain-mae:3.97729\teval-mae:3.97729\n",
      "[4]\ttrain-mae:3.57956\teval-mae:3.57956\n",
      "[5]\ttrain-mae:3.22161\teval-mae:3.22161\n",
      "[6]\ttrain-mae:2.89945\teval-mae:2.89945\n",
      "[7]\ttrain-mae:2.6095\teval-mae:2.60951\n",
      "[8]\ttrain-mae:2.34856\teval-mae:2.34856\n",
      "[9]\ttrain-mae:2.1137\teval-mae:2.1137\n",
      "[10]\ttrain-mae:1.90233\teval-mae:1.90233\n",
      "[11]\ttrain-mae:1.7121\teval-mae:1.7121\n",
      "[12]\ttrain-mae:1.54089\teval-mae:1.54089\n",
      "[13]\ttrain-mae:1.3868\teval-mae:1.3868\n",
      "[14]\ttrain-mae:1.24812\teval-mae:1.24812\n",
      "[15]\ttrain-mae:1.12331\teval-mae:1.12331\n",
      "[16]\ttrain-mae:1.01098\teval-mae:1.01098\n",
      "[17]\ttrain-mae:0.909884\teval-mae:0.909885\n",
      "[18]\ttrain-mae:0.818897\teval-mae:0.818897\n",
      "[19]\ttrain-mae:0.737008\teval-mae:0.737008\n",
      "[20]\ttrain-mae:0.663307\teval-mae:0.663307\n",
      "[21]\ttrain-mae:0.596977\teval-mae:0.596977\n",
      "[22]\ttrain-mae:0.53728\teval-mae:0.53728\n",
      "[23]\ttrain-mae:0.483552\teval-mae:0.483552\n",
      "[24]\ttrain-mae:0.435197\teval-mae:0.435197\n",
      "[25]\ttrain-mae:0.391677\teval-mae:0.391678\n",
      "[26]\ttrain-mae:0.35251\teval-mae:0.35251\n",
      "[27]\ttrain-mae:0.317259\teval-mae:0.31726\n",
      "[28]\ttrain-mae:0.285533\teval-mae:0.285534\n",
      "[29]\ttrain-mae:0.25698\teval-mae:0.256981\n",
      "[30]\ttrain-mae:0.231283\teval-mae:0.231283\n",
      "[31]\ttrain-mae:0.208155\teval-mae:0.208155\n",
      "[32]\ttrain-mae:0.187339\teval-mae:0.18734\n",
      "[33]\ttrain-mae:0.168605\teval-mae:0.168606\n",
      "[34]\ttrain-mae:0.151745\teval-mae:0.151745\n",
      "[35]\ttrain-mae:0.13657\teval-mae:0.136571\n",
      "[36]\ttrain-mae:0.122913\teval-mae:0.122914\n",
      "[37]\ttrain-mae:0.110622\teval-mae:0.110622\n",
      "[38]\ttrain-mae:0.09956\teval-mae:0.09956\n",
      "[39]\ttrain-mae:0.089604\teval-mae:0.089604\n",
      "[40]\ttrain-mae:0.080644\teval-mae:0.080644\n",
      "[41]\ttrain-mae:0.072579\teval-mae:0.07258\n",
      "[42]\ttrain-mae:0.065321\teval-mae:0.065322\n",
      "[43]\ttrain-mae:0.058789\teval-mae:0.058789\n",
      "[44]\ttrain-mae:0.05291\teval-mae:0.052911\n",
      "[45]\ttrain-mae:0.047619\teval-mae:0.04762\n",
      "[46]\ttrain-mae:0.042858\teval-mae:0.042858\n",
      "[47]\ttrain-mae:0.038572\teval-mae:0.038572\n",
      "[48]\ttrain-mae:0.034715\teval-mae:0.034715\n",
      "[49]\ttrain-mae:0.031243\teval-mae:0.031244\n",
      "[50]\ttrain-mae:0.028119\teval-mae:0.028119\n",
      "[51]\ttrain-mae:0.025307\teval-mae:0.025307\n",
      "[52]\ttrain-mae:0.022777\teval-mae:0.022777\n",
      "[53]\ttrain-mae:0.020499\teval-mae:0.020499\n",
      "[54]\ttrain-mae:0.018449\teval-mae:0.018449\n",
      "[55]\ttrain-mae:0.016604\teval-mae:0.016604\n",
      "[56]\ttrain-mae:0.014944\teval-mae:0.014944\n",
      "[57]\ttrain-mae:0.013449\teval-mae:0.013449\n",
      "[58]\ttrain-mae:0.012104\teval-mae:0.012105\n",
      "[59]\ttrain-mae:0.010894\teval-mae:0.010895\n",
      "[60]\ttrain-mae:0.009805\teval-mae:0.009805\n",
      "[61]\ttrain-mae:0.008824\teval-mae:0.008825\n",
      "[62]\ttrain-mae:0.007942\teval-mae:0.007942\n",
      "[63]\ttrain-mae:0.007148\teval-mae:0.007148\n",
      "[64]\ttrain-mae:0.006433\teval-mae:0.006433\n",
      "[65]\ttrain-mae:0.00579\teval-mae:0.00579\n",
      "[66]\ttrain-mae:0.005211\teval-mae:0.005212\n",
      "[67]\ttrain-mae:0.00469\teval-mae:0.004691\n",
      "[68]\ttrain-mae:0.004221\teval-mae:0.004222\n",
      "[69]\ttrain-mae:0.003799\teval-mae:0.003801\n",
      "[70]\ttrain-mae:0.00342\teval-mae:0.003421\n",
      "[71]\ttrain-mae:0.003079\teval-mae:0.003081\n",
      "[72]\ttrain-mae:0.002774\teval-mae:0.002775\n",
      "[73]\ttrain-mae:0.0025\teval-mae:0.002502\n",
      "[74]\ttrain-mae:0.002256\teval-mae:0.002258\n",
      "[75]\ttrain-mae:0.002041\teval-mae:0.002044\n",
      "[76]\ttrain-mae:0.001854\teval-mae:0.001857\n",
      "[77]\ttrain-mae:0.001693\teval-mae:0.001696\n",
      "[78]\ttrain-mae:0.001556\teval-mae:0.001559\n",
      "[79]\ttrain-mae:0.001442\teval-mae:0.001445\n",
      "[80]\ttrain-mae:0.001348\teval-mae:0.00135\n",
      "[81]\ttrain-mae:0.00127\teval-mae:0.001271\n",
      "[82]\ttrain-mae:0.001205\teval-mae:0.001207\n",
      "[83]\ttrain-mae:0.001153\teval-mae:0.001155\n",
      "[84]\ttrain-mae:0.00111\teval-mae:0.001112\n",
      "[85]\ttrain-mae:0.001075\teval-mae:0.001077\n",
      "[86]\ttrain-mae:0.001047\teval-mae:0.001048\n",
      "[87]\ttrain-mae:0.001024\teval-mae:0.001025\n",
      "[88]\ttrain-mae:0.001005\teval-mae:0.001007\n",
      "[89]\ttrain-mae:0.00099\teval-mae:0.000992\n",
      "[90]\ttrain-mae:0.000978\teval-mae:0.00098\n",
      "[91]\ttrain-mae:0.000968\teval-mae:0.000971\n",
      "[92]\ttrain-mae:0.000961\teval-mae:0.000963\n",
      "[93]\ttrain-mae:0.000955\teval-mae:0.000957\n",
      "[94]\ttrain-mae:0.00095\teval-mae:0.000953\n",
      "[95]\ttrain-mae:0.000946\teval-mae:0.000949\n",
      "[96]\ttrain-mae:0.000943\teval-mae:0.000946\n",
      "[97]\ttrain-mae:0.00094\teval-mae:0.000944\n",
      "[98]\ttrain-mae:0.000938\teval-mae:0.000942\n",
      "[99]\ttrain-mae:0.000937\teval-mae:0.00094\n",
      "[100]\ttrain-mae:0.000935\teval-mae:0.000939\n",
      "[101]\ttrain-mae:0.000934\teval-mae:0.000939\n",
      "[102]\ttrain-mae:0.000934\teval-mae:0.000938\n",
      "[103]\ttrain-mae:0.000933\teval-mae:0.000937\n",
      "[104]\ttrain-mae:0.000933\teval-mae:0.000937\n",
      "[105]\ttrain-mae:0.000932\teval-mae:0.000937\n",
      "[106]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[107]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[108]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[109]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[110]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[111]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[112]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[113]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[114]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[115]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[116]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[117]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[118]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[119]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[120]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[121]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[122]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[123]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[124]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[125]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[126]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[127]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[128]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[129]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[130]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[131]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "Stopping. Best iteration:\n",
      "[106]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "\n",
      " eval-MAE: 0.662727\n",
      "[0]\ttrain-mae:5.4558\teval-mae:5.45579\n",
      "Multiple eval metrics have been passed: 'eval-mae' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mae hasn't improved in 25 rounds.\n",
      "[1]\ttrain-mae:4.91023\teval-mae:4.91022\n",
      "[2]\ttrain-mae:4.41921\teval-mae:4.4192\n",
      "[3]\ttrain-mae:3.97729\teval-mae:3.97728\n",
      "[4]\ttrain-mae:3.57956\teval-mae:3.57955\n",
      "[5]\ttrain-mae:3.22161\teval-mae:3.2216\n",
      "[6]\ttrain-mae:2.89945\teval-mae:2.89944\n",
      "[7]\ttrain-mae:2.60951\teval-mae:2.6095\n",
      "[8]\ttrain-mae:2.34856\teval-mae:2.34855\n",
      "[9]\ttrain-mae:2.1137\teval-mae:2.11369\n",
      "[10]\ttrain-mae:1.90233\teval-mae:1.90232\n",
      "[11]\ttrain-mae:1.7121\teval-mae:1.71209\n",
      "[12]\ttrain-mae:1.54089\teval-mae:1.54088\n",
      "[13]\ttrain-mae:1.3868\teval-mae:1.38679\n",
      "[14]\ttrain-mae:1.24812\teval-mae:1.24811\n",
      "[15]\ttrain-mae:1.12331\teval-mae:1.1233\n",
      "[16]\ttrain-mae:1.01098\teval-mae:1.01097\n",
      "[17]\ttrain-mae:0.909884\teval-mae:0.909874\n",
      "[18]\ttrain-mae:0.818897\teval-mae:0.818887\n",
      "[19]\ttrain-mae:0.737008\teval-mae:0.736998\n",
      "[20]\ttrain-mae:0.663307\teval-mae:0.663297\n",
      "[21]\ttrain-mae:0.596977\teval-mae:0.596967\n",
      "[22]\ttrain-mae:0.53728\teval-mae:0.53727\n",
      "[23]\ttrain-mae:0.483552\teval-mae:0.483542\n",
      "[24]\ttrain-mae:0.435197\teval-mae:0.435187\n",
      "[25]\ttrain-mae:0.391677\teval-mae:0.391667\n",
      "[26]\ttrain-mae:0.35251\teval-mae:0.3525\n",
      "[27]\ttrain-mae:0.317259\teval-mae:0.317249\n",
      "[28]\ttrain-mae:0.285533\teval-mae:0.285523\n",
      "[29]\ttrain-mae:0.25698\teval-mae:0.25697\n",
      "[30]\ttrain-mae:0.231283\teval-mae:0.231273\n",
      "[31]\ttrain-mae:0.208155\teval-mae:0.208145\n",
      "[32]\ttrain-mae:0.187339\teval-mae:0.187329\n",
      "[33]\ttrain-mae:0.168605\teval-mae:0.168595\n",
      "[34]\ttrain-mae:0.151745\teval-mae:0.151735\n",
      "[35]\ttrain-mae:0.13657\teval-mae:0.13656\n",
      "[36]\ttrain-mae:0.122913\teval-mae:0.122903\n",
      "[37]\ttrain-mae:0.110622\teval-mae:0.110612\n",
      "[38]\ttrain-mae:0.09956\teval-mae:0.09955\n",
      "[39]\ttrain-mae:0.089604\teval-mae:0.089594\n",
      "[40]\ttrain-mae:0.080644\teval-mae:0.080634\n",
      "[41]\ttrain-mae:0.072579\teval-mae:0.072569\n",
      "[42]\ttrain-mae:0.065321\teval-mae:0.065311\n",
      "[43]\ttrain-mae:0.058789\teval-mae:0.058779\n",
      "[44]\ttrain-mae:0.05291\teval-mae:0.0529\n",
      "[45]\ttrain-mae:0.047619\teval-mae:0.047609\n",
      "[46]\ttrain-mae:0.042858\teval-mae:0.042848\n",
      "[47]\ttrain-mae:0.038572\teval-mae:0.038562\n",
      "[48]\ttrain-mae:0.034715\teval-mae:0.034705\n",
      "[49]\ttrain-mae:0.031243\teval-mae:0.031233\n",
      "[50]\ttrain-mae:0.028119\teval-mae:0.028109\n",
      "[51]\ttrain-mae:0.025307\teval-mae:0.025297\n",
      "[52]\ttrain-mae:0.022777\teval-mae:0.022767\n",
      "[53]\ttrain-mae:0.020499\teval-mae:0.020489\n",
      "[54]\ttrain-mae:0.018449\teval-mae:0.018439\n",
      "[55]\ttrain-mae:0.016604\teval-mae:0.016594\n",
      "[56]\ttrain-mae:0.014944\teval-mae:0.014934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57]\ttrain-mae:0.013449\teval-mae:0.013439\n",
      "[58]\ttrain-mae:0.012105\teval-mae:0.012094\n",
      "[59]\ttrain-mae:0.010894\teval-mae:0.010884\n",
      "[60]\ttrain-mae:0.009805\teval-mae:0.009795\n",
      "[61]\ttrain-mae:0.008824\teval-mae:0.008814\n",
      "[62]\ttrain-mae:0.007942\teval-mae:0.007932\n",
      "[63]\ttrain-mae:0.007148\teval-mae:0.007138\n",
      "[64]\ttrain-mae:0.006433\teval-mae:0.006423\n",
      "[65]\ttrain-mae:0.00579\teval-mae:0.00578\n",
      "[66]\ttrain-mae:0.005211\teval-mae:0.005201\n",
      "[67]\ttrain-mae:0.00469\teval-mae:0.00468\n",
      "[68]\ttrain-mae:0.004221\teval-mae:0.004211\n",
      "[69]\ttrain-mae:0.0038\teval-mae:0.003789\n",
      "[70]\ttrain-mae:0.00342\teval-mae:0.003409\n",
      "[71]\ttrain-mae:0.00308\teval-mae:0.003068\n",
      "[72]\ttrain-mae:0.002774\teval-mae:0.002762\n",
      "[73]\ttrain-mae:0.0025\teval-mae:0.002488\n",
      "[74]\ttrain-mae:0.002257\teval-mae:0.002245\n",
      "[75]\ttrain-mae:0.002042\teval-mae:0.00203\n",
      "[76]\ttrain-mae:0.001854\teval-mae:0.001843\n",
      "[77]\ttrain-mae:0.001693\teval-mae:0.001682\n",
      "[78]\ttrain-mae:0.001557\teval-mae:0.001547\n",
      "[79]\ttrain-mae:0.001443\teval-mae:0.001433\n",
      "[80]\ttrain-mae:0.001348\teval-mae:0.00134\n",
      "[81]\ttrain-mae:0.00127\teval-mae:0.001263\n",
      "[82]\ttrain-mae:0.001206\teval-mae:0.001199\n",
      "[83]\ttrain-mae:0.001153\teval-mae:0.001148\n",
      "[84]\ttrain-mae:0.00111\teval-mae:0.001106\n",
      "[85]\ttrain-mae:0.001075\teval-mae:0.001072\n",
      "[86]\ttrain-mae:0.001047\teval-mae:0.001044\n",
      "[87]\ttrain-mae:0.001024\teval-mae:0.001022\n",
      "[88]\ttrain-mae:0.001005\teval-mae:0.001004\n",
      "[89]\ttrain-mae:0.00099\teval-mae:0.000989\n",
      "[90]\ttrain-mae:0.000978\teval-mae:0.000978\n",
      "[91]\ttrain-mae:0.000969\teval-mae:0.000968\n",
      "[92]\ttrain-mae:0.000961\teval-mae:0.000961\n",
      "[93]\ttrain-mae:0.000955\teval-mae:0.000955\n",
      "[94]\ttrain-mae:0.00095\teval-mae:0.00095\n",
      "[95]\ttrain-mae:0.000946\teval-mae:0.000946\n",
      "[96]\ttrain-mae:0.000943\teval-mae:0.000944\n",
      "[97]\ttrain-mae:0.00094\teval-mae:0.000941\n",
      "[98]\ttrain-mae:0.000938\teval-mae:0.000939\n",
      "[99]\ttrain-mae:0.000937\teval-mae:0.000938\n",
      "[100]\ttrain-mae:0.000936\teval-mae:0.000937\n",
      "[101]\ttrain-mae:0.000935\teval-mae:0.000936\n",
      "[102]\ttrain-mae:0.000934\teval-mae:0.000935\n",
      "[103]\ttrain-mae:0.000933\teval-mae:0.000935\n",
      "[104]\ttrain-mae:0.000933\teval-mae:0.000934\n",
      "[105]\ttrain-mae:0.000933\teval-mae:0.000934\n",
      "[106]\ttrain-mae:0.000932\teval-mae:0.000934\n",
      "[107]\ttrain-mae:0.000932\teval-mae:0.000934\n",
      "[108]\ttrain-mae:0.000932\teval-mae:0.000934\n",
      "[109]\ttrain-mae:0.000932\teval-mae:0.000934\n",
      "[110]\ttrain-mae:0.000932\teval-mae:0.000934\n",
      "[111]\ttrain-mae:0.000932\teval-mae:0.000934\n",
      "[112]\ttrain-mae:0.000932\teval-mae:0.000934\n",
      "[113]\ttrain-mae:0.000932\teval-mae:0.000934\n",
      "[114]\ttrain-mae:0.000932\teval-mae:0.000934\n",
      "[115]\ttrain-mae:0.000932\teval-mae:0.000934\n",
      "[116]\ttrain-mae:0.000932\teval-mae:0.000934\n",
      "[117]\ttrain-mae:0.000932\teval-mae:0.000934\n",
      "[118]\ttrain-mae:0.000932\teval-mae:0.000934\n",
      "[119]\ttrain-mae:0.000932\teval-mae:0.000934\n",
      "[120]\ttrain-mae:0.000932\teval-mae:0.000934\n",
      "[121]\ttrain-mae:0.000932\teval-mae:0.000934\n",
      "[122]\ttrain-mae:0.000932\teval-mae:0.000934\n",
      "[123]\ttrain-mae:0.000932\teval-mae:0.000934\n",
      "[124]\ttrain-mae:0.000932\teval-mae:0.000934\n",
      "[125]\ttrain-mae:0.000932\teval-mae:0.000934\n",
      "[126]\ttrain-mae:0.000932\teval-mae:0.000934\n",
      "[127]\ttrain-mae:0.000932\teval-mae:0.000934\n",
      "[128]\ttrain-mae:0.000932\teval-mae:0.000934\n",
      "[129]\ttrain-mae:0.000932\teval-mae:0.000934\n",
      "Stopping. Best iteration:\n",
      "[104]\ttrain-mae:0.000933\teval-mae:0.000934\n",
      "\n",
      " eval-MAE: 0.661319\n",
      "[0]\ttrain-mae:5.4558\teval-mae:5.4558\n",
      "Multiple eval metrics have been passed: 'eval-mae' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mae hasn't improved in 25 rounds.\n",
      "[1]\ttrain-mae:4.91022\teval-mae:4.91023\n",
      "[2]\ttrain-mae:4.41921\teval-mae:4.41921\n",
      "[3]\ttrain-mae:3.97729\teval-mae:3.97729\n",
      "[4]\ttrain-mae:3.57956\teval-mae:3.57956\n",
      "[5]\ttrain-mae:3.22161\teval-mae:3.22161\n",
      "[6]\ttrain-mae:2.89945\teval-mae:2.89945\n",
      "[7]\ttrain-mae:2.6095\teval-mae:2.60951\n",
      "[8]\ttrain-mae:2.34856\teval-mae:2.34856\n",
      "[9]\ttrain-mae:2.1137\teval-mae:2.1137\n",
      "[10]\ttrain-mae:1.90233\teval-mae:1.90234\n",
      "[11]\ttrain-mae:1.7121\teval-mae:1.7121\n",
      "[12]\ttrain-mae:1.54089\teval-mae:1.54089\n",
      "[13]\ttrain-mae:1.3868\teval-mae:1.38681\n",
      "[14]\ttrain-mae:1.24812\teval-mae:1.24813\n",
      "[15]\ttrain-mae:1.12331\teval-mae:1.12331\n",
      "[16]\ttrain-mae:1.01098\teval-mae:1.01098\n",
      "[17]\ttrain-mae:0.909884\teval-mae:0.909886\n",
      "[18]\ttrain-mae:0.818896\teval-mae:0.818898\n",
      "[19]\ttrain-mae:0.737007\teval-mae:0.737009\n",
      "[20]\ttrain-mae:0.663307\teval-mae:0.663309\n",
      "[21]\ttrain-mae:0.596977\teval-mae:0.596978\n",
      "[22]\ttrain-mae:0.537279\teval-mae:0.537281\n",
      "[23]\ttrain-mae:0.483552\teval-mae:0.483554\n",
      "[24]\ttrain-mae:0.435197\teval-mae:0.435199\n",
      "[25]\ttrain-mae:0.391677\teval-mae:0.391679\n",
      "[26]\ttrain-mae:0.35251\teval-mae:0.352512\n",
      "[27]\ttrain-mae:0.317259\teval-mae:0.317261\n",
      "[28]\ttrain-mae:0.285533\teval-mae:0.285535\n",
      "[29]\ttrain-mae:0.25698\teval-mae:0.256982\n",
      "[30]\ttrain-mae:0.231282\teval-mae:0.231284\n",
      "[31]\ttrain-mae:0.208154\teval-mae:0.208156\n",
      "[32]\ttrain-mae:0.187339\teval-mae:0.187341\n",
      "[33]\ttrain-mae:0.168605\teval-mae:0.168607\n",
      "[34]\ttrain-mae:0.151745\teval-mae:0.151747\n",
      "[35]\ttrain-mae:0.13657\teval-mae:0.136572\n",
      "[36]\ttrain-mae:0.122913\teval-mae:0.122915\n",
      "[37]\ttrain-mae:0.110622\teval-mae:0.110624\n",
      "[38]\ttrain-mae:0.09956\teval-mae:0.099561\n",
      "[39]\ttrain-mae:0.089604\teval-mae:0.089606\n",
      "[40]\ttrain-mae:0.080643\teval-mae:0.080645\n",
      "[41]\ttrain-mae:0.072579\teval-mae:0.072581\n",
      "[42]\ttrain-mae:0.065321\teval-mae:0.065323\n",
      "[43]\ttrain-mae:0.058789\teval-mae:0.058791\n",
      "[44]\ttrain-mae:0.05291\teval-mae:0.052912\n",
      "[45]\ttrain-mae:0.047619\teval-mae:0.047621\n",
      "[46]\ttrain-mae:0.042857\teval-mae:0.042859\n",
      "[47]\ttrain-mae:0.038572\teval-mae:0.038574\n",
      "[48]\ttrain-mae:0.034714\teval-mae:0.034716\n",
      "[49]\ttrain-mae:0.031243\teval-mae:0.031245\n",
      "[50]\ttrain-mae:0.028119\teval-mae:0.028121\n",
      "[51]\ttrain-mae:0.025307\teval-mae:0.025309\n",
      "[52]\ttrain-mae:0.022776\teval-mae:0.022778\n",
      "[53]\ttrain-mae:0.020499\teval-mae:0.020501\n",
      "[54]\ttrain-mae:0.018449\teval-mae:0.018451\n",
      "[55]\ttrain-mae:0.016604\teval-mae:0.016606\n",
      "[56]\ttrain-mae:0.014944\teval-mae:0.014946\n",
      "[57]\ttrain-mae:0.013449\teval-mae:0.013451\n",
      "[58]\ttrain-mae:0.012104\teval-mae:0.012106\n",
      "[59]\ttrain-mae:0.010894\teval-mae:0.010896\n",
      "[60]\ttrain-mae:0.009805\teval-mae:0.009806\n",
      "[61]\ttrain-mae:0.008824\teval-mae:0.008826\n",
      "[62]\ttrain-mae:0.007942\teval-mae:0.007943\n",
      "[63]\ttrain-mae:0.007148\teval-mae:0.00715\n",
      "[64]\ttrain-mae:0.006433\teval-mae:0.006435\n",
      "[65]\ttrain-mae:0.00579\teval-mae:0.005792\n",
      "[66]\ttrain-mae:0.005211\teval-mae:0.005213\n",
      "[67]\ttrain-mae:0.00469\teval-mae:0.004692\n",
      "[68]\ttrain-mae:0.004221\teval-mae:0.004224\n",
      "[69]\ttrain-mae:0.0038\teval-mae:0.003802\n",
      "[70]\ttrain-mae:0.00342\teval-mae:0.003423\n",
      "[71]\ttrain-mae:0.003079\teval-mae:0.003083\n",
      "[72]\ttrain-mae:0.002774\teval-mae:0.002778\n",
      "[73]\ttrain-mae:0.0025\teval-mae:0.002504\n",
      "[74]\ttrain-mae:0.002257\teval-mae:0.002261\n",
      "[75]\ttrain-mae:0.002042\teval-mae:0.002046\n",
      "[76]\ttrain-mae:0.001854\teval-mae:0.001857\n",
      "[77]\ttrain-mae:0.001693\teval-mae:0.001697\n",
      "[78]\ttrain-mae:0.001557\teval-mae:0.00156\n",
      "[79]\ttrain-mae:0.001443\teval-mae:0.001447\n",
      "[80]\ttrain-mae:0.001348\teval-mae:0.001352\n",
      "[81]\ttrain-mae:0.00127\teval-mae:0.001274\n",
      "[82]\ttrain-mae:0.001205\teval-mae:0.00121\n",
      "[83]\ttrain-mae:0.001153\teval-mae:0.001158\n",
      "[84]\ttrain-mae:0.00111\teval-mae:0.001115\n",
      "[85]\ttrain-mae:0.001075\teval-mae:0.00108\n",
      "[86]\ttrain-mae:0.001047\teval-mae:0.001051\n",
      "[87]\ttrain-mae:0.001024\teval-mae:0.001028\n",
      "[88]\ttrain-mae:0.001005\teval-mae:0.00101\n",
      "[89]\ttrain-mae:0.00099\teval-mae:0.000995\n",
      "[90]\ttrain-mae:0.000978\teval-mae:0.000982\n",
      "[91]\ttrain-mae:0.000968\teval-mae:0.000973\n",
      "[92]\ttrain-mae:0.000961\teval-mae:0.000965\n",
      "[93]\ttrain-mae:0.000954\teval-mae:0.000959\n",
      "[94]\ttrain-mae:0.00095\teval-mae:0.000954\n",
      "[95]\ttrain-mae:0.000946\teval-mae:0.00095\n",
      "[96]\ttrain-mae:0.000943\teval-mae:0.000947\n",
      "[97]\ttrain-mae:0.00094\teval-mae:0.000944\n",
      "[98]\ttrain-mae:0.000938\teval-mae:0.000942\n",
      "[99]\ttrain-mae:0.000937\teval-mae:0.000941\n",
      "[100]\ttrain-mae:0.000935\teval-mae:0.00094\n",
      "[101]\ttrain-mae:0.000934\teval-mae:0.000939\n",
      "[102]\ttrain-mae:0.000934\teval-mae:0.000938\n",
      "[103]\ttrain-mae:0.000933\teval-mae:0.000937\n",
      "[104]\ttrain-mae:0.000933\teval-mae:0.000937\n",
      "[105]\ttrain-mae:0.000932\teval-mae:0.000937\n",
      "[106]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[107]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[108]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[109]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[110]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[111]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[112]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[113]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[114]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[115]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[116]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[117]\ttrain-mae:0.000932\teval-mae:0.000936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[118]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[119]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[120]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[121]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[122]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[123]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[124]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[125]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[126]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[127]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[128]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[129]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[130]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[131]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "Stopping. Best iteration:\n",
      "[106]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "\n",
      " eval-MAE: 0.662645\n",
      "[0]\ttrain-mae:5.4558\teval-mae:5.45581\n",
      "Multiple eval metrics have been passed: 'eval-mae' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mae hasn't improved in 25 rounds.\n",
      "[1]\ttrain-mae:4.91022\teval-mae:4.91023\n",
      "[2]\ttrain-mae:4.4192\teval-mae:4.41921\n",
      "[3]\ttrain-mae:3.97729\teval-mae:3.97729\n",
      "[4]\ttrain-mae:3.57956\teval-mae:3.57957\n",
      "[5]\ttrain-mae:3.22161\teval-mae:3.22161\n",
      "[6]\ttrain-mae:2.89945\teval-mae:2.89945\n",
      "[7]\ttrain-mae:2.6095\teval-mae:2.60951\n",
      "[8]\ttrain-mae:2.34856\teval-mae:2.34856\n",
      "[9]\ttrain-mae:2.1137\teval-mae:2.11371\n",
      "[10]\ttrain-mae:1.90233\teval-mae:1.90234\n",
      "[11]\ttrain-mae:1.7121\teval-mae:1.7121\n",
      "[12]\ttrain-mae:1.54089\teval-mae:1.5409\n",
      "[13]\ttrain-mae:1.3868\teval-mae:1.38681\n",
      "[14]\ttrain-mae:1.24812\teval-mae:1.24813\n",
      "[15]\ttrain-mae:1.12331\teval-mae:1.12332\n",
      "[16]\ttrain-mae:1.01098\teval-mae:1.01099\n",
      "[17]\ttrain-mae:0.909884\teval-mae:0.909889\n",
      "[18]\ttrain-mae:0.818896\teval-mae:0.818901\n",
      "[19]\ttrain-mae:0.737007\teval-mae:0.737012\n",
      "[20]\ttrain-mae:0.663307\teval-mae:0.663311\n",
      "[21]\ttrain-mae:0.596976\teval-mae:0.596981\n",
      "[22]\ttrain-mae:0.537279\teval-mae:0.537284\n",
      "[23]\ttrain-mae:0.483551\teval-mae:0.483556\n",
      "[24]\ttrain-mae:0.435196\teval-mae:0.435201\n",
      "[25]\ttrain-mae:0.391677\teval-mae:0.391682\n",
      "[26]\ttrain-mae:0.352509\teval-mae:0.352514\n",
      "[27]\ttrain-mae:0.317259\teval-mae:0.317263\n",
      "[28]\ttrain-mae:0.285533\teval-mae:0.285538\n",
      "[29]\ttrain-mae:0.25698\teval-mae:0.256985\n",
      "[30]\ttrain-mae:0.231282\teval-mae:0.231287\n",
      "[31]\ttrain-mae:0.208154\teval-mae:0.208159\n",
      "[32]\ttrain-mae:0.187339\teval-mae:0.187343\n",
      "[33]\ttrain-mae:0.168605\teval-mae:0.168609\n",
      "[34]\ttrain-mae:0.151744\teval-mae:0.151749\n",
      "[35]\ttrain-mae:0.13657\teval-mae:0.136575\n",
      "[36]\ttrain-mae:0.122913\teval-mae:0.122918\n",
      "[37]\ttrain-mae:0.110621\teval-mae:0.110626\n",
      "[38]\ttrain-mae:0.099559\teval-mae:0.099564\n",
      "[39]\ttrain-mae:0.089603\teval-mae:0.089608\n",
      "[40]\ttrain-mae:0.080643\teval-mae:0.080648\n",
      "[41]\ttrain-mae:0.072579\teval-mae:0.072584\n",
      "[42]\ttrain-mae:0.065321\teval-mae:0.065326\n",
      "[43]\ttrain-mae:0.058789\teval-mae:0.058793\n",
      "[44]\ttrain-mae:0.05291\teval-mae:0.052915\n",
      "[45]\ttrain-mae:0.047619\teval-mae:0.047624\n",
      "[46]\ttrain-mae:0.042857\teval-mae:0.042862\n",
      "[47]\ttrain-mae:0.038571\teval-mae:0.038576\n",
      "[48]\ttrain-mae:0.034714\teval-mae:0.034719\n",
      "[49]\ttrain-mae:0.031243\teval-mae:0.031247\n",
      "[50]\ttrain-mae:0.028119\teval-mae:0.028123\n",
      "[51]\ttrain-mae:0.025307\teval-mae:0.025311\n",
      "[52]\ttrain-mae:0.022776\teval-mae:0.022781\n",
      "[53]\ttrain-mae:0.020499\teval-mae:0.020503\n",
      "[54]\ttrain-mae:0.018449\teval-mae:0.018453\n",
      "[55]\ttrain-mae:0.016604\teval-mae:0.016609\n",
      "[56]\ttrain-mae:0.014944\teval-mae:0.014948\n",
      "[57]\ttrain-mae:0.013449\teval-mae:0.013454\n",
      "[58]\ttrain-mae:0.012104\teval-mae:0.012109\n",
      "[59]\ttrain-mae:0.010894\teval-mae:0.010899\n",
      "[60]\ttrain-mae:0.009805\teval-mae:0.009809\n",
      "[61]\ttrain-mae:0.008824\teval-mae:0.008829\n",
      "[62]\ttrain-mae:0.007942\teval-mae:0.007946\n",
      "[63]\ttrain-mae:0.007148\teval-mae:0.007152\n",
      "[64]\ttrain-mae:0.006433\teval-mae:0.006438\n",
      "[65]\ttrain-mae:0.00579\teval-mae:0.005794\n",
      "[66]\ttrain-mae:0.005211\teval-mae:0.005216\n",
      "[67]\ttrain-mae:0.00469\teval-mae:0.004695\n",
      "[68]\ttrain-mae:0.004221\teval-mae:0.004226\n",
      "[69]\ttrain-mae:0.0038\teval-mae:0.003804\n",
      "[70]\ttrain-mae:0.00342\teval-mae:0.003424\n",
      "[71]\ttrain-mae:0.003079\teval-mae:0.003083\n",
      "[72]\ttrain-mae:0.002774\teval-mae:0.002777\n",
      "[73]\ttrain-mae:0.0025\teval-mae:0.002503\n",
      "[74]\ttrain-mae:0.002257\teval-mae:0.00226\n",
      "[75]\ttrain-mae:0.002042\teval-mae:0.002045\n",
      "[76]\ttrain-mae:0.001854\teval-mae:0.001857\n",
      "[77]\ttrain-mae:0.001693\teval-mae:0.001696\n",
      "[78]\ttrain-mae:0.001557\teval-mae:0.001559\n",
      "[79]\ttrain-mae:0.001443\teval-mae:0.001444\n",
      "[80]\ttrain-mae:0.001348\teval-mae:0.001348\n",
      "[81]\ttrain-mae:0.00127\teval-mae:0.001269\n",
      "[82]\ttrain-mae:0.001206\teval-mae:0.001204\n",
      "[83]\ttrain-mae:0.001154\teval-mae:0.001151\n",
      "[84]\ttrain-mae:0.001111\teval-mae:0.001108\n",
      "[85]\ttrain-mae:0.001076\teval-mae:0.001073\n",
      "[86]\ttrain-mae:0.001047\teval-mae:0.001044\n",
      "[87]\ttrain-mae:0.001024\teval-mae:0.001021\n",
      "[88]\ttrain-mae:0.001006\teval-mae:0.001002\n",
      "[89]\ttrain-mae:0.000991\teval-mae:0.000986\n",
      "[90]\ttrain-mae:0.000979\teval-mae:0.000974\n",
      "[91]\ttrain-mae:0.000969\teval-mae:0.000964\n",
      "[92]\ttrain-mae:0.000962\teval-mae:0.000956\n",
      "[93]\ttrain-mae:0.000956\teval-mae:0.00095\n",
      "[94]\ttrain-mae:0.000951\teval-mae:0.000945\n",
      "[95]\ttrain-mae:0.000947\teval-mae:0.00094\n",
      "[96]\ttrain-mae:0.000944\teval-mae:0.000937\n",
      "[97]\ttrain-mae:0.000941\teval-mae:0.000935\n",
      "[98]\ttrain-mae:0.000939\teval-mae:0.000933\n",
      "[99]\ttrain-mae:0.000938\teval-mae:0.000931\n",
      "[100]\ttrain-mae:0.000936\teval-mae:0.00093\n",
      "[101]\ttrain-mae:0.000936\teval-mae:0.000929\n",
      "[102]\ttrain-mae:0.000935\teval-mae:0.000928\n",
      "[103]\ttrain-mae:0.000934\teval-mae:0.000928\n",
      "[104]\ttrain-mae:0.000934\teval-mae:0.000927\n",
      "[105]\ttrain-mae:0.000934\teval-mae:0.000927\n",
      "[106]\ttrain-mae:0.000933\teval-mae:0.000927\n",
      "[107]\ttrain-mae:0.000933\teval-mae:0.000926\n",
      "[108]\ttrain-mae:0.000933\teval-mae:0.000926\n",
      "[109]\ttrain-mae:0.000933\teval-mae:0.000926\n",
      "[110]\ttrain-mae:0.000933\teval-mae:0.000926\n",
      "[111]\ttrain-mae:0.000933\teval-mae:0.000926\n",
      "[112]\ttrain-mae:0.000933\teval-mae:0.000926\n",
      "[113]\ttrain-mae:0.000933\teval-mae:0.000926\n",
      "[114]\ttrain-mae:0.000933\teval-mae:0.000926\n",
      "[115]\ttrain-mae:0.000933\teval-mae:0.000926\n",
      "[116]\ttrain-mae:0.000933\teval-mae:0.000926\n",
      "[117]\ttrain-mae:0.000933\teval-mae:0.000926\n",
      "[118]\ttrain-mae:0.000933\teval-mae:0.000926\n",
      "[119]\ttrain-mae:0.000933\teval-mae:0.000926\n",
      "[120]\ttrain-mae:0.000933\teval-mae:0.000926\n",
      "[121]\ttrain-mae:0.000933\teval-mae:0.000926\n",
      "[122]\ttrain-mae:0.000933\teval-mae:0.000926\n",
      "[123]\ttrain-mae:0.000933\teval-mae:0.000926\n",
      "[124]\ttrain-mae:0.000933\teval-mae:0.000926\n",
      "[125]\ttrain-mae:0.000933\teval-mae:0.000926\n",
      "[126]\ttrain-mae:0.000933\teval-mae:0.000927\n",
      "[127]\ttrain-mae:0.000933\teval-mae:0.000927\n",
      "[128]\ttrain-mae:0.000933\teval-mae:0.000927\n",
      "[129]\ttrain-mae:0.000933\teval-mae:0.000927\n",
      "[130]\ttrain-mae:0.000933\teval-mae:0.000927\n",
      "[131]\ttrain-mae:0.000933\teval-mae:0.000927\n",
      "[132]\ttrain-mae:0.000933\teval-mae:0.000927\n",
      "Stopping. Best iteration:\n",
      "[107]\ttrain-mae:0.000933\teval-mae:0.000926\n",
      "\n",
      " eval-MAE: 0.655695\n",
      "[0]\ttrain-mae:5.4558\teval-mae:5.45581\n",
      "Multiple eval metrics have been passed: 'eval-mae' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mae hasn't improved in 25 rounds.\n",
      "[1]\ttrain-mae:4.91022\teval-mae:4.91023\n",
      "[2]\ttrain-mae:4.4192\teval-mae:4.41921\n",
      "[3]\ttrain-mae:3.97729\teval-mae:3.97729\n",
      "[4]\ttrain-mae:3.57956\teval-mae:3.57957\n",
      "[5]\ttrain-mae:3.22161\teval-mae:3.22161\n",
      "[6]\ttrain-mae:2.89945\teval-mae:2.89945\n",
      "[7]\ttrain-mae:2.6095\teval-mae:2.60951\n",
      "[8]\ttrain-mae:2.34856\teval-mae:2.34856\n",
      "[9]\ttrain-mae:2.1137\teval-mae:2.11371\n",
      "[10]\ttrain-mae:1.90233\teval-mae:1.90234\n",
      "[11]\ttrain-mae:1.7121\teval-mae:1.71211\n",
      "[12]\ttrain-mae:1.54089\teval-mae:1.5409\n",
      "[13]\ttrain-mae:1.3868\teval-mae:1.38681\n",
      "[14]\ttrain-mae:1.24812\teval-mae:1.24813\n",
      "[15]\ttrain-mae:1.12331\teval-mae:1.12332\n",
      "[16]\ttrain-mae:1.01098\teval-mae:1.01099\n",
      "[17]\ttrain-mae:0.909884\teval-mae:0.90989\n",
      "[18]\ttrain-mae:0.818896\teval-mae:0.818902\n",
      "[19]\ttrain-mae:0.737007\teval-mae:0.737013\n",
      "[20]\ttrain-mae:0.663307\teval-mae:0.663312\n",
      "[21]\ttrain-mae:0.596976\teval-mae:0.596982\n",
      "[22]\ttrain-mae:0.537279\teval-mae:0.537285\n",
      "[23]\ttrain-mae:0.483551\teval-mae:0.483557\n",
      "[24]\ttrain-mae:0.435196\teval-mae:0.435202\n",
      "[25]\ttrain-mae:0.391677\teval-mae:0.391682\n",
      "[26]\ttrain-mae:0.352509\teval-mae:0.352515\n",
      "[27]\ttrain-mae:0.317259\teval-mae:0.317264\n",
      "[28]\ttrain-mae:0.285533\teval-mae:0.285539\n",
      "[29]\ttrain-mae:0.25698\teval-mae:0.256985\n",
      "[30]\ttrain-mae:0.231282\teval-mae:0.231288\n",
      "[31]\ttrain-mae:0.208154\teval-mae:0.20816\n",
      "[32]\ttrain-mae:0.187339\teval-mae:0.187344\n",
      "[33]\ttrain-mae:0.168605\teval-mae:0.16861\n",
      "[34]\ttrain-mae:0.151744\teval-mae:0.15175\n",
      "[35]\ttrain-mae:0.13657\teval-mae:0.136576\n",
      "[36]\ttrain-mae:0.122913\teval-mae:0.122918\n",
      "[37]\ttrain-mae:0.110621\teval-mae:0.110627\n",
      "[38]\ttrain-mae:0.099559\teval-mae:0.099565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[39]\ttrain-mae:0.089603\teval-mae:0.089609\n",
      "[40]\ttrain-mae:0.080643\teval-mae:0.080649\n",
      "[41]\ttrain-mae:0.072579\teval-mae:0.072584\n",
      "[42]\ttrain-mae:0.065321\teval-mae:0.065327\n",
      "[43]\ttrain-mae:0.058789\teval-mae:0.058794\n",
      "[44]\ttrain-mae:0.05291\teval-mae:0.052915\n",
      "[45]\ttrain-mae:0.047619\teval-mae:0.047624\n",
      "[46]\ttrain-mae:0.042857\teval-mae:0.042863\n",
      "[47]\ttrain-mae:0.038571\teval-mae:0.038577\n",
      "[48]\ttrain-mae:0.034714\teval-mae:0.03472\n",
      "[49]\ttrain-mae:0.031243\teval-mae:0.031248\n",
      "[50]\ttrain-mae:0.028118\teval-mae:0.028124\n",
      "[51]\ttrain-mae:0.025307\teval-mae:0.025312\n",
      "[52]\ttrain-mae:0.022776\teval-mae:0.022782\n",
      "[53]\ttrain-mae:0.020499\teval-mae:0.020504\n",
      "[54]\ttrain-mae:0.018449\teval-mae:0.018454\n",
      "[55]\ttrain-mae:0.016604\teval-mae:0.016609\n",
      "[56]\ttrain-mae:0.014943\teval-mae:0.014949\n",
      "[57]\ttrain-mae:0.013449\teval-mae:0.013455\n",
      "[58]\ttrain-mae:0.012104\teval-mae:0.01211\n",
      "[59]\ttrain-mae:0.010894\teval-mae:0.0109\n",
      "[60]\ttrain-mae:0.009805\teval-mae:0.00981\n",
      "[61]\ttrain-mae:0.008824\teval-mae:0.00883\n",
      "[62]\ttrain-mae:0.007942\teval-mae:0.007947\n",
      "[63]\ttrain-mae:0.007148\teval-mae:0.007153\n",
      "[64]\ttrain-mae:0.006433\teval-mae:0.006439\n",
      "[65]\ttrain-mae:0.00579\teval-mae:0.005796\n",
      "[66]\ttrain-mae:0.005211\teval-mae:0.005217\n",
      "[67]\ttrain-mae:0.00469\teval-mae:0.004696\n",
      "[68]\ttrain-mae:0.004221\teval-mae:0.004227\n",
      "[69]\ttrain-mae:0.003799\teval-mae:0.003805\n",
      "[70]\ttrain-mae:0.00342\teval-mae:0.003426\n",
      "[71]\ttrain-mae:0.003079\teval-mae:0.003085\n",
      "[72]\ttrain-mae:0.002774\teval-mae:0.002779\n",
      "[73]\ttrain-mae:0.0025\teval-mae:0.002505\n",
      "[74]\ttrain-mae:0.002257\teval-mae:0.002262\n",
      "[75]\ttrain-mae:0.002041\teval-mae:0.002046\n",
      "[76]\ttrain-mae:0.001854\teval-mae:0.001859\n",
      "[77]\ttrain-mae:0.001693\teval-mae:0.001698\n",
      "[78]\ttrain-mae:0.001557\teval-mae:0.001562\n",
      "[79]\ttrain-mae:0.001442\teval-mae:0.001448\n",
      "[80]\ttrain-mae:0.001348\teval-mae:0.001353\n",
      "[81]\ttrain-mae:0.001269\teval-mae:0.001275\n",
      "[82]\ttrain-mae:0.001205\teval-mae:0.001212\n",
      "[83]\ttrain-mae:0.001153\teval-mae:0.001159\n",
      "[84]\ttrain-mae:0.00111\teval-mae:0.001117\n",
      "[85]\ttrain-mae:0.001075\teval-mae:0.001082\n",
      "[86]\ttrain-mae:0.001046\teval-mae:0.001054\n",
      "[87]\ttrain-mae:0.001023\teval-mae:0.001031\n",
      "[88]\ttrain-mae:0.001005\teval-mae:0.001012\n",
      "[89]\ttrain-mae:0.00099\teval-mae:0.000997\n",
      "[90]\ttrain-mae:0.000978\teval-mae:0.000985\n",
      "[91]\ttrain-mae:0.000968\teval-mae:0.000975\n",
      "[92]\ttrain-mae:0.000961\teval-mae:0.000967\n",
      "[93]\ttrain-mae:0.000954\teval-mae:0.000961\n",
      "[94]\ttrain-mae:0.000949\teval-mae:0.000956\n",
      "[95]\ttrain-mae:0.000946\teval-mae:0.000952\n",
      "[96]\ttrain-mae:0.000942\teval-mae:0.000949\n",
      "[97]\ttrain-mae:0.00094\teval-mae:0.000946\n",
      "[98]\ttrain-mae:0.000938\teval-mae:0.000944\n",
      "[99]\ttrain-mae:0.000937\teval-mae:0.000942\n",
      "[100]\ttrain-mae:0.000935\teval-mae:0.000941\n",
      "[101]\ttrain-mae:0.000934\teval-mae:0.00094\n",
      "[102]\ttrain-mae:0.000934\teval-mae:0.000939\n",
      "[103]\ttrain-mae:0.000933\teval-mae:0.000938\n",
      "[104]\ttrain-mae:0.000933\teval-mae:0.000938\n",
      "[105]\ttrain-mae:0.000932\teval-mae:0.000937\n",
      "[106]\ttrain-mae:0.000932\teval-mae:0.000937\n",
      "[107]\ttrain-mae:0.000932\teval-mae:0.000937\n",
      "[108]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[109]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[110]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[111]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[112]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[113]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[114]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[115]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[116]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[117]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[118]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[119]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[120]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[121]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[122]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[123]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[124]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[125]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[126]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[127]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[128]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[129]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[130]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[131]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[132]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "[133]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "Stopping. Best iteration:\n",
      "[108]\ttrain-mae:0.000932\teval-mae:0.000936\n",
      "\n",
      " eval-MAE: 0.662773\n",
      "[0]\ttrain-mae:5.4558\teval-mae:5.4558\n",
      "Multiple eval metrics have been passed: 'eval-mae' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mae hasn't improved in 25 rounds.\n",
      "[1]\ttrain-mae:4.91022\teval-mae:4.91022\n",
      "[2]\ttrain-mae:4.41921\teval-mae:4.4192\n",
      "[3]\ttrain-mae:3.97729\teval-mae:3.97729\n",
      "[4]\ttrain-mae:3.57956\teval-mae:3.57956\n",
      "[5]\ttrain-mae:3.22161\teval-mae:3.22161\n",
      "[6]\ttrain-mae:2.89945\teval-mae:2.89945\n",
      "[7]\ttrain-mae:2.6095\teval-mae:2.6095\n",
      "[8]\ttrain-mae:2.34856\teval-mae:2.34856\n",
      "[9]\ttrain-mae:2.1137\teval-mae:2.1137\n",
      "[10]\ttrain-mae:1.90233\teval-mae:1.90233\n",
      "[11]\ttrain-mae:1.7121\teval-mae:1.7121\n",
      "[12]\ttrain-mae:1.54089\teval-mae:1.54089\n",
      "[13]\ttrain-mae:1.3868\teval-mae:1.3868\n",
      "[14]\ttrain-mae:1.24812\teval-mae:1.24812\n",
      "[15]\ttrain-mae:1.12331\teval-mae:1.12331\n",
      "[16]\ttrain-mae:1.01098\teval-mae:1.01098\n",
      "[17]\ttrain-mae:0.909885\teval-mae:0.909884\n",
      "[18]\ttrain-mae:0.818897\teval-mae:0.818896\n",
      "[19]\ttrain-mae:0.737008\teval-mae:0.737007\n",
      "[20]\ttrain-mae:0.663307\teval-mae:0.663306\n",
      "[21]\ttrain-mae:0.596977\teval-mae:0.596976\n",
      "[22]\ttrain-mae:0.53728\teval-mae:0.537279\n",
      "[23]\ttrain-mae:0.483552\teval-mae:0.483551\n",
      "[24]\ttrain-mae:0.435197\teval-mae:0.435196\n",
      "[25]\ttrain-mae:0.391677\teval-mae:0.391676\n",
      "[26]\ttrain-mae:0.35251\teval-mae:0.352509\n",
      "[27]\ttrain-mae:0.317259\teval-mae:0.317258\n",
      "[28]\ttrain-mae:0.285533\teval-mae:0.285533\n",
      "[29]\ttrain-mae:0.25698\teval-mae:0.25698\n",
      "[30]\ttrain-mae:0.231283\teval-mae:0.231282\n",
      "[31]\ttrain-mae:0.208155\teval-mae:0.208154\n",
      "[32]\ttrain-mae:0.187339\teval-mae:0.187338\n",
      "[33]\ttrain-mae:0.168605\teval-mae:0.168604\n",
      "[34]\ttrain-mae:0.151745\teval-mae:0.151744\n",
      "[35]\ttrain-mae:0.13657\teval-mae:0.13657\n",
      "[36]\ttrain-mae:0.122913\teval-mae:0.122912\n",
      "[37]\ttrain-mae:0.110622\teval-mae:0.110621\n",
      "[38]\ttrain-mae:0.09956\teval-mae:0.099559\n",
      "[39]\ttrain-mae:0.089604\teval-mae:0.089603\n",
      "[40]\ttrain-mae:0.080644\teval-mae:0.080643\n",
      "[41]\ttrain-mae:0.072579\teval-mae:0.072578\n",
      "[42]\ttrain-mae:0.065321\teval-mae:0.065321\n",
      "[43]\ttrain-mae:0.058789\teval-mae:0.058788\n",
      "[44]\ttrain-mae:0.05291\teval-mae:0.052909\n",
      "[45]\ttrain-mae:0.047619\teval-mae:0.047618\n",
      "[46]\ttrain-mae:0.042858\teval-mae:0.042857\n",
      "[47]\ttrain-mae:0.038572\teval-mae:0.038571\n",
      "[48]\ttrain-mae:0.034715\teval-mae:0.034714\n",
      "[49]\ttrain-mae:0.031243\teval-mae:0.031242\n",
      "[50]\ttrain-mae:0.028119\teval-mae:0.028118\n",
      "[51]\ttrain-mae:0.025307\teval-mae:0.025306\n",
      "[52]\ttrain-mae:0.022777\teval-mae:0.022776\n",
      "[53]\ttrain-mae:0.020499\teval-mae:0.020498\n",
      "[54]\ttrain-mae:0.018449\teval-mae:0.018448\n",
      "[55]\ttrain-mae:0.016604\teval-mae:0.016603\n",
      "[56]\ttrain-mae:0.014944\teval-mae:0.014943\n",
      "[57]\ttrain-mae:0.013449\teval-mae:0.013448\n",
      "[58]\ttrain-mae:0.012105\teval-mae:0.012104\n",
      "[59]\ttrain-mae:0.010894\teval-mae:0.010893\n",
      "[60]\ttrain-mae:0.009805\teval-mae:0.009804\n",
      "[61]\ttrain-mae:0.008824\teval-mae:0.008823\n",
      "[62]\ttrain-mae:0.007942\teval-mae:0.007941\n",
      "[63]\ttrain-mae:0.007148\teval-mae:0.007147\n",
      "[64]\ttrain-mae:0.006433\teval-mae:0.006432\n",
      "[65]\ttrain-mae:0.00579\teval-mae:0.005789\n",
      "[66]\ttrain-mae:0.005211\teval-mae:0.005211\n",
      "[67]\ttrain-mae:0.00469\teval-mae:0.00469\n",
      "[68]\ttrain-mae:0.004221\teval-mae:0.004221\n",
      "[69]\ttrain-mae:0.0038\teval-mae:0.003799\n",
      "[70]\ttrain-mae:0.00342\teval-mae:0.00342\n",
      "[71]\ttrain-mae:0.003079\teval-mae:0.003079\n",
      "[72]\ttrain-mae:0.002774\teval-mae:0.002774\n",
      "[73]\ttrain-mae:0.0025\teval-mae:0.0025\n",
      "[74]\ttrain-mae:0.002257\teval-mae:0.002257\n",
      "[75]\ttrain-mae:0.002042\teval-mae:0.002042\n",
      "[76]\ttrain-mae:0.001854\teval-mae:0.001855\n",
      "[77]\ttrain-mae:0.001693\teval-mae:0.001693\n",
      "[78]\ttrain-mae:0.001557\teval-mae:0.001557\n",
      "[79]\ttrain-mae:0.001443\teval-mae:0.001443\n",
      "[80]\ttrain-mae:0.001348\teval-mae:0.001348\n",
      "[81]\ttrain-mae:0.00127\teval-mae:0.00127\n",
      "[82]\ttrain-mae:0.001206\teval-mae:0.001206\n",
      "[83]\ttrain-mae:0.001153\teval-mae:0.001153\n",
      "[84]\ttrain-mae:0.00111\teval-mae:0.00111\n",
      "[85]\ttrain-mae:0.001075\teval-mae:0.001076\n",
      "[86]\ttrain-mae:0.001047\teval-mae:0.001047\n",
      "[87]\ttrain-mae:0.001024\teval-mae:0.001024\n",
      "[88]\ttrain-mae:0.001005\teval-mae:0.001006\n",
      "[89]\ttrain-mae:0.00099\teval-mae:0.000991\n",
      "[90]\ttrain-mae:0.000978\teval-mae:0.000979\n",
      "[91]\ttrain-mae:0.000969\teval-mae:0.000969\n",
      "[92]\ttrain-mae:0.000961\teval-mae:0.000961\n",
      "[93]\ttrain-mae:0.000955\teval-mae:0.000955\n",
      "[94]\ttrain-mae:0.00095\teval-mae:0.00095\n",
      "[95]\ttrain-mae:0.000946\teval-mae:0.000946\n",
      "[96]\ttrain-mae:0.000943\teval-mae:0.000943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[97]\ttrain-mae:0.000941\teval-mae:0.00094\n",
      "[98]\ttrain-mae:0.000939\teval-mae:0.000938\n",
      "[99]\ttrain-mae:0.000937\teval-mae:0.000937\n",
      "[100]\ttrain-mae:0.000936\teval-mae:0.000936\n",
      "[101]\ttrain-mae:0.000935\teval-mae:0.000935\n",
      "[102]\ttrain-mae:0.000934\teval-mae:0.000934\n",
      "[103]\ttrain-mae:0.000934\teval-mae:0.000933\n",
      "[104]\ttrain-mae:0.000933\teval-mae:0.000933\n",
      "[105]\ttrain-mae:0.000933\teval-mae:0.000933\n",
      "[106]\ttrain-mae:0.000933\teval-mae:0.000932\n",
      "[107]\ttrain-mae:0.000932\teval-mae:0.000932\n",
      "[108]\ttrain-mae:0.000932\teval-mae:0.000932\n",
      "[109]\ttrain-mae:0.000932\teval-mae:0.000932\n",
      "[110]\ttrain-mae:0.000932\teval-mae:0.000932\n",
      "[111]\ttrain-mae:0.000932\teval-mae:0.000932\n",
      "[112]\ttrain-mae:0.000932\teval-mae:0.000932\n",
      "[113]\ttrain-mae:0.000932\teval-mae:0.000932\n",
      "[114]\ttrain-mae:0.000932\teval-mae:0.000932\n",
      "[115]\ttrain-mae:0.000932\teval-mae:0.000932\n",
      "[116]\ttrain-mae:0.000932\teval-mae:0.000932\n",
      "[117]\ttrain-mae:0.000932\teval-mae:0.000932\n",
      "[118]\ttrain-mae:0.000932\teval-mae:0.000932\n",
      "[119]\ttrain-mae:0.000932\teval-mae:0.000932\n",
      "[120]\ttrain-mae:0.000932\teval-mae:0.000932\n",
      "[121]\ttrain-mae:0.000932\teval-mae:0.000932\n",
      "[122]\ttrain-mae:0.000932\teval-mae:0.000932\n",
      "[123]\ttrain-mae:0.000932\teval-mae:0.000932\n",
      "[124]\ttrain-mae:0.000932\teval-mae:0.000932\n",
      "[125]\ttrain-mae:0.000932\teval-mae:0.000932\n",
      "[126]\ttrain-mae:0.000932\teval-mae:0.000932\n",
      "[127]\ttrain-mae:0.000932\teval-mae:0.000932\n",
      "[128]\ttrain-mae:0.000932\teval-mae:0.000932\n",
      "[129]\ttrain-mae:0.000932\teval-mae:0.000933\n",
      "[130]\ttrain-mae:0.000932\teval-mae:0.000933\n",
      "[131]\ttrain-mae:0.000932\teval-mae:0.000933\n",
      "Stopping. Best iteration:\n",
      "[106]\ttrain-mae:0.000933\teval-mae:0.000932\n",
      "\n",
      " eval-MAE: 0.659891\n",
      "[0]\ttrain-mae:5.4558\teval-mae:5.4558\n",
      "Multiple eval metrics have been passed: 'eval-mae' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mae hasn't improved in 25 rounds.\n",
      "[1]\ttrain-mae:4.91022\teval-mae:4.91023\n",
      "[2]\ttrain-mae:4.41921\teval-mae:4.41921\n",
      "[3]\ttrain-mae:3.97729\teval-mae:3.97729\n",
      "[4]\ttrain-mae:3.57956\teval-mae:3.57956\n",
      "[5]\ttrain-mae:3.22161\teval-mae:3.22161\n",
      "[6]\ttrain-mae:2.89945\teval-mae:2.89945\n",
      "[7]\ttrain-mae:2.6095\teval-mae:2.60951\n",
      "[8]\ttrain-mae:2.34856\teval-mae:2.34856\n",
      "[9]\ttrain-mae:2.1137\teval-mae:2.1137\n",
      "[10]\ttrain-mae:1.90233\teval-mae:1.90234\n",
      "[11]\ttrain-mae:1.7121\teval-mae:1.7121\n",
      "[12]\ttrain-mae:1.54089\teval-mae:1.54089\n",
      "[13]\ttrain-mae:1.3868\teval-mae:1.38681\n",
      "[14]\ttrain-mae:1.24812\teval-mae:1.24813\n",
      "[15]\ttrain-mae:1.12331\teval-mae:1.12331\n",
      "[16]\ttrain-mae:1.01098\teval-mae:1.01098\n",
      "[17]\ttrain-mae:0.909884\teval-mae:0.909886\n",
      "[18]\ttrain-mae:0.818896\teval-mae:0.818898\n",
      "[19]\ttrain-mae:0.737007\teval-mae:0.737009\n",
      "[20]\ttrain-mae:0.663307\teval-mae:0.663309\n",
      "[21]\ttrain-mae:0.596977\teval-mae:0.596978\n",
      "[22]\ttrain-mae:0.537279\teval-mae:0.537281\n",
      "[23]\ttrain-mae:0.483552\teval-mae:0.483554\n",
      "[24]\ttrain-mae:0.435197\teval-mae:0.435198\n",
      "[25]\ttrain-mae:0.391677\teval-mae:0.391679\n",
      "[26]\ttrain-mae:0.35251\teval-mae:0.352512\n",
      "[27]\ttrain-mae:0.317259\teval-mae:0.317261\n",
      "[28]\ttrain-mae:0.285533\teval-mae:0.285535\n",
      "[29]\ttrain-mae:0.25698\teval-mae:0.256982\n",
      "[30]\ttrain-mae:0.231282\teval-mae:0.231284\n",
      "[31]\ttrain-mae:0.208154\teval-mae:0.208156\n",
      "[32]\ttrain-mae:0.187339\teval-mae:0.187341\n",
      "[33]\ttrain-mae:0.168605\teval-mae:0.168607\n",
      "[34]\ttrain-mae:0.151745\teval-mae:0.151746\n",
      "[35]\ttrain-mae:0.13657\teval-mae:0.136572\n",
      "[36]\ttrain-mae:0.122913\teval-mae:0.122915\n",
      "[37]\ttrain-mae:0.110622\teval-mae:0.110624\n",
      "[38]\ttrain-mae:0.09956\teval-mae:0.099561\n",
      "[39]\ttrain-mae:0.089604\teval-mae:0.089606\n",
      "[40]\ttrain-mae:0.080643\teval-mae:0.080645\n",
      "[41]\ttrain-mae:0.072579\teval-mae:0.072581\n",
      "[42]\ttrain-mae:0.065321\teval-mae:0.065323\n",
      "[43]\ttrain-mae:0.058789\teval-mae:0.058791\n",
      "[44]\ttrain-mae:0.05291\teval-mae:0.052912\n",
      "[45]\ttrain-mae:0.047619\teval-mae:0.047621\n",
      "[46]\ttrain-mae:0.042857\teval-mae:0.042859\n",
      "[47]\ttrain-mae:0.038572\teval-mae:0.038573\n",
      "[48]\ttrain-mae:0.034714\teval-mae:0.034716\n",
      "[49]\ttrain-mae:0.031243\teval-mae:0.031245\n",
      "[50]\ttrain-mae:0.028119\teval-mae:0.028121\n",
      "[51]\ttrain-mae:0.025307\teval-mae:0.025309\n",
      "[52]\ttrain-mae:0.022776\teval-mae:0.022778\n",
      "[53]\ttrain-mae:0.020499\teval-mae:0.020501\n",
      "[54]\ttrain-mae:0.018449\teval-mae:0.018451\n",
      "[55]\ttrain-mae:0.016604\teval-mae:0.016606\n",
      "[56]\ttrain-mae:0.014944\teval-mae:0.014946\n",
      "[57]\ttrain-mae:0.013449\teval-mae:0.013451\n",
      "[58]\ttrain-mae:0.012104\teval-mae:0.012106\n",
      "[59]\ttrain-mae:0.010894\teval-mae:0.010896\n",
      "[60]\ttrain-mae:0.009805\teval-mae:0.009806\n",
      "[61]\ttrain-mae:0.008824\teval-mae:0.008826\n",
      "[62]\ttrain-mae:0.007942\teval-mae:0.007943\n",
      "[63]\ttrain-mae:0.007148\teval-mae:0.007149\n",
      "[64]\ttrain-mae:0.006433\teval-mae:0.006435\n",
      "[65]\ttrain-mae:0.00579\teval-mae:0.005791\n",
      "[66]\ttrain-mae:0.005211\teval-mae:0.005212\n",
      "[67]\ttrain-mae:0.00469\teval-mae:0.004691\n",
      "[68]\ttrain-mae:0.004221\teval-mae:0.004223\n",
      "[69]\ttrain-mae:0.0038\teval-mae:0.003801\n",
      "[70]\ttrain-mae:0.003421\teval-mae:0.003422\n",
      "[71]\ttrain-mae:0.00308\teval-mae:0.003081\n",
      "[72]\ttrain-mae:0.002774\teval-mae:0.002775\n",
      "[73]\ttrain-mae:0.0025\teval-mae:0.002501\n",
      "[74]\ttrain-mae:0.002257\teval-mae:0.002258\n",
      "[75]\ttrain-mae:0.002042\teval-mae:0.002043\n",
      "[76]\ttrain-mae:0.001854\teval-mae:0.001856\n",
      "[77]\ttrain-mae:0.001693\teval-mae:0.001696\n",
      "[78]\ttrain-mae:0.001557\teval-mae:0.00156\n",
      "[79]\ttrain-mae:0.001443\teval-mae:0.001446\n",
      "[80]\ttrain-mae:0.001348\teval-mae:0.001352\n",
      "[81]\ttrain-mae:0.00127\teval-mae:0.001274\n",
      "[82]\ttrain-mae:0.001206\teval-mae:0.001209\n",
      "[83]\ttrain-mae:0.001153\teval-mae:0.001156\n",
      "[84]\ttrain-mae:0.00111\teval-mae:0.001113\n",
      "[85]\ttrain-mae:0.001075\teval-mae:0.001078\n",
      "[86]\ttrain-mae:0.001047\teval-mae:0.001049\n",
      "[87]\ttrain-mae:0.001024\teval-mae:0.001026\n",
      "[88]\ttrain-mae:0.001005\teval-mae:0.001008\n",
      "[89]\ttrain-mae:0.00099\teval-mae:0.000993\n",
      "[90]\ttrain-mae:0.000978\teval-mae:0.000981\n",
      "[91]\ttrain-mae:0.000969\teval-mae:0.000971\n",
      "[92]\ttrain-mae:0.000961\teval-mae:0.000963\n",
      "[93]\ttrain-mae:0.000955\teval-mae:0.000957\n",
      "[94]\ttrain-mae:0.00095\teval-mae:0.000952\n",
      "[95]\ttrain-mae:0.000946\teval-mae:0.000948\n",
      "[96]\ttrain-mae:0.000943\teval-mae:0.000945\n",
      "[97]\ttrain-mae:0.00094\teval-mae:0.000943\n",
      "[98]\ttrain-mae:0.000938\teval-mae:0.000941\n",
      "[99]\ttrain-mae:0.000937\teval-mae:0.000939\n",
      "[100]\ttrain-mae:0.000936\teval-mae:0.000938\n",
      "[101]\ttrain-mae:0.000935\teval-mae:0.000937\n",
      "[102]\ttrain-mae:0.000934\teval-mae:0.000936\n",
      "[103]\ttrain-mae:0.000933\teval-mae:0.000936\n",
      "[104]\ttrain-mae:0.000933\teval-mae:0.000935\n",
      "[105]\ttrain-mae:0.000933\teval-mae:0.000935\n",
      "[106]\ttrain-mae:0.000932\teval-mae:0.000935\n",
      "[107]\ttrain-mae:0.000932\teval-mae:0.000935\n",
      "[108]\ttrain-mae:0.000932\teval-mae:0.000934\n",
      "[109]\ttrain-mae:0.000932\teval-mae:0.000934\n",
      "[110]\ttrain-mae:0.000932\teval-mae:0.000934\n",
      "[111]\ttrain-mae:0.000932\teval-mae:0.000934\n",
      "[112]\ttrain-mae:0.000932\teval-mae:0.000934\n",
      "[113]\ttrain-mae:0.000932\teval-mae:0.000934\n",
      "[114]\ttrain-mae:0.000932\teval-mae:0.000934\n",
      "[115]\ttrain-mae:0.000932\teval-mae:0.000934\n",
      "[116]\ttrain-mae:0.000932\teval-mae:0.000934\n",
      "[117]\ttrain-mae:0.000932\teval-mae:0.000934\n",
      "[118]\ttrain-mae:0.000932\teval-mae:0.000935\n",
      "[119]\ttrain-mae:0.000932\teval-mae:0.000935\n",
      "[120]\ttrain-mae:0.000932\teval-mae:0.000935\n",
      "[121]\ttrain-mae:0.000932\teval-mae:0.000935\n",
      "[122]\ttrain-mae:0.000932\teval-mae:0.000935\n",
      "[123]\ttrain-mae:0.000932\teval-mae:0.000935\n",
      "[124]\ttrain-mae:0.000932\teval-mae:0.000935\n",
      "[125]\ttrain-mae:0.000932\teval-mae:0.000935\n",
      "[126]\ttrain-mae:0.000932\teval-mae:0.000935\n",
      "[127]\ttrain-mae:0.000932\teval-mae:0.000935\n",
      "[128]\ttrain-mae:0.000932\teval-mae:0.000935\n",
      "[129]\ttrain-mae:0.000932\teval-mae:0.000935\n",
      "[130]\ttrain-mae:0.000932\teval-mae:0.000935\n",
      "[131]\ttrain-mae:0.000932\teval-mae:0.000935\n",
      "[132]\ttrain-mae:0.000932\teval-mae:0.000935\n",
      "[133]\ttrain-mae:0.000932\teval-mae:0.000935\n",
      "Stopping. Best iteration:\n",
      "[108]\ttrain-mae:0.000932\teval-mae:0.000934\n",
      "\n",
      " eval-MAE: 0.661347\n",
      "[0]\ttrain-mae:5.4558\teval-mae:5.4558\n",
      "Multiple eval metrics have been passed: 'eval-mae' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mae hasn't improved in 25 rounds.\n",
      "[1]\ttrain-mae:4.91022\teval-mae:4.91022\n",
      "[2]\ttrain-mae:4.41921\teval-mae:4.4192\n",
      "[3]\ttrain-mae:3.97729\teval-mae:3.97729\n",
      "[4]\ttrain-mae:3.57956\teval-mae:3.57956\n",
      "[5]\ttrain-mae:3.22161\teval-mae:3.2216\n",
      "[6]\ttrain-mae:2.89945\teval-mae:2.89945\n",
      "[7]\ttrain-mae:2.6095\teval-mae:2.6095\n",
      "[8]\ttrain-mae:2.34856\teval-mae:2.34855\n",
      "[9]\ttrain-mae:2.1137\teval-mae:2.1137\n",
      "[10]\ttrain-mae:1.90233\teval-mae:1.90233\n",
      "[11]\ttrain-mae:1.7121\teval-mae:1.7121\n",
      "[12]\ttrain-mae:1.54089\teval-mae:1.54089\n",
      "[13]\ttrain-mae:1.3868\teval-mae:1.3868\n",
      "[14]\ttrain-mae:1.24812\teval-mae:1.24812\n",
      "[15]\ttrain-mae:1.12331\teval-mae:1.12331\n",
      "[16]\ttrain-mae:1.01098\teval-mae:1.01098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17]\ttrain-mae:0.909884\teval-mae:0.909881\n",
      "[18]\ttrain-mae:0.818897\teval-mae:0.818894\n",
      "[19]\ttrain-mae:0.737007\teval-mae:0.737005\n",
      "[20]\ttrain-mae:0.663307\teval-mae:0.663304\n",
      "[21]\ttrain-mae:0.596976\teval-mae:0.596974\n",
      "[22]\ttrain-mae:0.537279\teval-mae:0.537277\n",
      "[23]\ttrain-mae:0.483552\teval-mae:0.483549\n",
      "[24]\ttrain-mae:0.435197\teval-mae:0.435194\n",
      "[25]\ttrain-mae:0.391677\teval-mae:0.391674\n",
      "[26]\ttrain-mae:0.35251\teval-mae:0.352507\n",
      "[27]\ttrain-mae:0.317259\teval-mae:0.317256\n",
      "[28]\ttrain-mae:0.285533\teval-mae:0.28553\n",
      "[29]\ttrain-mae:0.25698\teval-mae:0.256977\n",
      "[30]\ttrain-mae:0.231282\teval-mae:0.23128\n",
      "[31]\ttrain-mae:0.208154\teval-mae:0.208152\n",
      "[32]\ttrain-mae:0.187339\teval-mae:0.187336\n",
      "[33]\ttrain-mae:0.168605\teval-mae:0.168602\n",
      "[34]\ttrain-mae:0.151745\teval-mae:0.151742\n",
      "[35]\ttrain-mae:0.13657\teval-mae:0.136568\n",
      "[36]\ttrain-mae:0.122913\teval-mae:0.12291\n",
      "[37]\ttrain-mae:0.110622\teval-mae:0.110619\n",
      "[38]\ttrain-mae:0.09956\teval-mae:0.099557\n",
      "[39]\ttrain-mae:0.089604\teval-mae:0.089601\n",
      "[40]\ttrain-mae:0.080643\teval-mae:0.080641\n",
      "[41]\ttrain-mae:0.072579\teval-mae:0.072576\n",
      "[42]\ttrain-mae:0.065321\teval-mae:0.065319\n",
      "[43]\ttrain-mae:0.058789\teval-mae:0.058786\n",
      "[44]\ttrain-mae:0.05291\teval-mae:0.052907\n",
      "[45]\ttrain-mae:0.047619\teval-mae:0.047616\n",
      "[46]\ttrain-mae:0.042857\teval-mae:0.042855\n",
      "[47]\ttrain-mae:0.038572\teval-mae:0.038569\n",
      "[48]\ttrain-mae:0.034714\teval-mae:0.034712\n",
      "[49]\ttrain-mae:0.031243\teval-mae:0.03124\n",
      "[50]\ttrain-mae:0.028119\teval-mae:0.028116\n",
      "[51]\ttrain-mae:0.025307\teval-mae:0.025304\n",
      "[52]\ttrain-mae:0.022776\teval-mae:0.022774\n",
      "[53]\ttrain-mae:0.020499\teval-mae:0.020496\n",
      "[54]\ttrain-mae:0.018449\teval-mae:0.018446\n",
      "[55]\ttrain-mae:0.016604\teval-mae:0.016601\n",
      "[56]\ttrain-mae:0.014944\teval-mae:0.014941\n",
      "[57]\ttrain-mae:0.013449\teval-mae:0.013447\n",
      "[58]\ttrain-mae:0.012105\teval-mae:0.012102\n",
      "[59]\ttrain-mae:0.010894\teval-mae:0.010891\n",
      "[60]\ttrain-mae:0.009804\teval-mae:0.009802\n",
      "[61]\ttrain-mae:0.008824\teval-mae:0.008822\n",
      "[62]\ttrain-mae:0.007941\teval-mae:0.007939\n",
      "[63]\ttrain-mae:0.007148\teval-mae:0.007145\n",
      "[64]\ttrain-mae:0.006433\teval-mae:0.006431\n",
      "[65]\ttrain-mae:0.00579\teval-mae:0.005787\n",
      "[66]\ttrain-mae:0.005211\teval-mae:0.005209\n",
      "[67]\ttrain-mae:0.00469\teval-mae:0.004688\n",
      "[68]\ttrain-mae:0.004221\teval-mae:0.004219\n",
      "[69]\ttrain-mae:0.0038\teval-mae:0.003798\n",
      "[70]\ttrain-mae:0.00342\teval-mae:0.003418\n",
      "[71]\ttrain-mae:0.00308\teval-mae:0.003077\n",
      "[72]\ttrain-mae:0.002774\teval-mae:0.002771\n",
      "[73]\ttrain-mae:0.0025\teval-mae:0.002498\n",
      "[74]\ttrain-mae:0.002257\teval-mae:0.002255\n",
      "[75]\ttrain-mae:0.002042\teval-mae:0.00204\n",
      "[76]\ttrain-mae:0.001854\teval-mae:0.001852\n",
      "[77]\ttrain-mae:0.001693\teval-mae:0.00169\n",
      "[78]\ttrain-mae:0.001557\teval-mae:0.001554\n",
      "[79]\ttrain-mae:0.001443\teval-mae:0.001439\n",
      "[80]\ttrain-mae:0.001348\teval-mae:0.001344\n",
      "[81]\ttrain-mae:0.00127\teval-mae:0.001266\n",
      "[82]\ttrain-mae:0.001206\teval-mae:0.001202\n",
      "[83]\ttrain-mae:0.001154\teval-mae:0.00115\n",
      "[84]\ttrain-mae:0.001111\teval-mae:0.001107\n",
      "[85]\ttrain-mae:0.001076\teval-mae:0.001072\n",
      "[86]\ttrain-mae:0.001047\teval-mae:0.001044\n",
      "[87]\ttrain-mae:0.001024\teval-mae:0.001021\n",
      "[88]\ttrain-mae:0.001006\teval-mae:0.001002\n",
      "[89]\ttrain-mae:0.000991\teval-mae:0.000987\n",
      "[90]\ttrain-mae:0.000979\teval-mae:0.000976\n",
      "[91]\ttrain-mae:0.000969\teval-mae:0.000966\n",
      "[92]\ttrain-mae:0.000961\teval-mae:0.000959\n",
      "[93]\ttrain-mae:0.000955\teval-mae:0.000953\n",
      "[94]\ttrain-mae:0.00095\teval-mae:0.000948\n",
      "[95]\ttrain-mae:0.000946\teval-mae:0.000944\n",
      "[96]\ttrain-mae:0.000943\teval-mae:0.000941\n",
      "[97]\ttrain-mae:0.000941\teval-mae:0.000939\n",
      "[98]\ttrain-mae:0.000939\teval-mae:0.000937\n",
      "[99]\ttrain-mae:0.000937\teval-mae:0.000935\n",
      "[100]\ttrain-mae:0.000936\teval-mae:0.000934\n",
      "[101]\ttrain-mae:0.000935\teval-mae:0.000933\n",
      "[102]\ttrain-mae:0.000934\teval-mae:0.000932\n",
      "[103]\ttrain-mae:0.000934\teval-mae:0.000932\n",
      "[104]\ttrain-mae:0.000933\teval-mae:0.000931\n",
      "[105]\ttrain-mae:0.000933\teval-mae:0.000931\n",
      "[106]\ttrain-mae:0.000933\teval-mae:0.000931\n",
      "[107]\ttrain-mae:0.000933\teval-mae:0.000931\n",
      "[108]\ttrain-mae:0.000932\teval-mae:0.000931\n",
      "[109]\ttrain-mae:0.000932\teval-mae:0.000931\n",
      "[110]\ttrain-mae:0.000932\teval-mae:0.000931\n",
      "[111]\ttrain-mae:0.000932\teval-mae:0.000931\n",
      "[112]\ttrain-mae:0.000932\teval-mae:0.000931\n",
      "[113]\ttrain-mae:0.000932\teval-mae:0.000931\n",
      "[114]\ttrain-mae:0.000932\teval-mae:0.000931\n",
      "[115]\ttrain-mae:0.000932\teval-mae:0.000931\n",
      "[116]\ttrain-mae:0.000932\teval-mae:0.000931\n",
      "[117]\ttrain-mae:0.000932\teval-mae:0.000931\n",
      "[118]\ttrain-mae:0.000932\teval-mae:0.000931\n",
      "[119]\ttrain-mae:0.000932\teval-mae:0.000931\n",
      "[120]\ttrain-mae:0.000932\teval-mae:0.000931\n",
      "[121]\ttrain-mae:0.000933\teval-mae:0.000931\n",
      "[122]\ttrain-mae:0.000933\teval-mae:0.000931\n",
      "[123]\ttrain-mae:0.000933\teval-mae:0.000931\n",
      "[124]\ttrain-mae:0.000933\teval-mae:0.000931\n",
      "[125]\ttrain-mae:0.000933\teval-mae:0.000931\n",
      "[126]\ttrain-mae:0.000933\teval-mae:0.000931\n",
      "[127]\ttrain-mae:0.000933\teval-mae:0.000931\n",
      "[128]\ttrain-mae:0.000933\teval-mae:0.000931\n",
      "[129]\ttrain-mae:0.000933\teval-mae:0.000931\n",
      "Stopping. Best iteration:\n",
      "[104]\ttrain-mae:0.000933\teval-mae:0.000931\n",
      "\n",
      " eval-MAE: 0.659158\n",
      "[0]\ttrain-mae:5.4558\teval-mae:5.45581\n",
      "Multiple eval metrics have been passed: 'eval-mae' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mae hasn't improved in 25 rounds.\n",
      "[1]\ttrain-mae:4.91022\teval-mae:4.91023\n",
      "[2]\ttrain-mae:4.4192\teval-mae:4.41921\n",
      "[3]\ttrain-mae:3.97729\teval-mae:3.97729\n",
      "[4]\ttrain-mae:3.57956\teval-mae:3.57957\n",
      "[5]\ttrain-mae:3.22161\teval-mae:3.22161\n",
      "[6]\ttrain-mae:2.89945\teval-mae:2.89946\n",
      "[7]\ttrain-mae:2.6095\teval-mae:2.60951\n",
      "[8]\ttrain-mae:2.34856\teval-mae:2.34856\n",
      "[9]\ttrain-mae:2.1137\teval-mae:2.11371\n",
      "[10]\ttrain-mae:1.90233\teval-mae:1.90234\n",
      "[11]\ttrain-mae:1.7121\teval-mae:1.71211\n",
      "[12]\ttrain-mae:1.54089\teval-mae:1.5409\n",
      "[13]\ttrain-mae:1.3868\teval-mae:1.38681\n",
      "[14]\ttrain-mae:1.24812\teval-mae:1.24813\n",
      "[15]\ttrain-mae:1.12331\teval-mae:1.12332\n",
      "[16]\ttrain-mae:1.01098\teval-mae:1.01099\n",
      "[17]\ttrain-mae:0.909884\teval-mae:0.909891\n",
      "[18]\ttrain-mae:0.818897\teval-mae:0.818903\n",
      "[19]\ttrain-mae:0.737007\teval-mae:0.737014\n",
      "[20]\ttrain-mae:0.663307\teval-mae:0.663314\n",
      "[21]\ttrain-mae:0.596976\teval-mae:0.596983\n",
      "[22]\ttrain-mae:0.537279\teval-mae:0.537286\n",
      "[23]\ttrain-mae:0.483552\teval-mae:0.483559\n",
      "[24]\ttrain-mae:0.435197\teval-mae:0.435203\n",
      "[25]\ttrain-mae:0.391677\teval-mae:0.391684\n",
      "[26]\ttrain-mae:0.35251\teval-mae:0.352517\n",
      "[27]\ttrain-mae:0.317259\teval-mae:0.317266\n",
      "[28]\ttrain-mae:0.285533\teval-mae:0.28554\n",
      "[29]\ttrain-mae:0.25698\teval-mae:0.256987\n",
      "[30]\ttrain-mae:0.231282\teval-mae:0.231289\n",
      "[31]\ttrain-mae:0.208154\teval-mae:0.208161\n",
      "[32]\ttrain-mae:0.187339\teval-mae:0.187346\n",
      "[33]\ttrain-mae:0.168605\teval-mae:0.168612\n",
      "[34]\ttrain-mae:0.151745\teval-mae:0.151751\n",
      "[35]\ttrain-mae:0.13657\teval-mae:0.136577\n",
      "[36]\ttrain-mae:0.122913\teval-mae:0.12292\n",
      "[37]\ttrain-mae:0.110622\teval-mae:0.110629\n",
      "[38]\ttrain-mae:0.09956\teval-mae:0.099566\n",
      "[39]\ttrain-mae:0.089604\teval-mae:0.08961\n",
      "[40]\ttrain-mae:0.080643\teval-mae:0.08065\n",
      "[41]\ttrain-mae:0.072579\teval-mae:0.072586\n",
      "[42]\ttrain-mae:0.065321\teval-mae:0.065328\n",
      "[43]\ttrain-mae:0.058789\teval-mae:0.058796\n",
      "[44]\ttrain-mae:0.05291\teval-mae:0.052917\n",
      "[45]\ttrain-mae:0.047619\teval-mae:0.047626\n",
      "[46]\ttrain-mae:0.042857\teval-mae:0.042864\n",
      "[47]\ttrain-mae:0.038572\teval-mae:0.038578\n",
      "[48]\ttrain-mae:0.034714\teval-mae:0.034721\n",
      "[49]\ttrain-mae:0.031243\teval-mae:0.03125\n",
      "[50]\ttrain-mae:0.028119\teval-mae:0.028126\n",
      "[51]\ttrain-mae:0.025307\teval-mae:0.025314\n",
      "[52]\ttrain-mae:0.022776\teval-mae:0.022783\n",
      "[53]\ttrain-mae:0.020499\teval-mae:0.020506\n",
      "[54]\ttrain-mae:0.018449\teval-mae:0.018456\n",
      "[55]\ttrain-mae:0.016604\teval-mae:0.016611\n",
      "[56]\ttrain-mae:0.014944\teval-mae:0.014951\n",
      "[57]\ttrain-mae:0.013449\teval-mae:0.013456\n",
      "[58]\ttrain-mae:0.012105\teval-mae:0.012112\n",
      "[59]\ttrain-mae:0.010894\teval-mae:0.010901\n",
      "[60]\ttrain-mae:0.009804\teval-mae:0.009811\n",
      "[61]\ttrain-mae:0.008824\teval-mae:0.008831\n",
      "[62]\ttrain-mae:0.007942\teval-mae:0.007948\n",
      "[63]\ttrain-mae:0.007148\teval-mae:0.007154\n",
      "[64]\ttrain-mae:0.006433\teval-mae:0.00644\n",
      "[65]\ttrain-mae:0.00579\teval-mae:0.005797\n",
      "[66]\ttrain-mae:0.005211\teval-mae:0.005218\n",
      "[67]\ttrain-mae:0.00469\teval-mae:0.004697\n",
      "[68]\ttrain-mae:0.004221\teval-mae:0.004228\n",
      "[69]\ttrain-mae:0.0038\teval-mae:0.003807\n",
      "[70]\ttrain-mae:0.00342\teval-mae:0.003427\n",
      "[71]\ttrain-mae:0.00308\teval-mae:0.003086\n",
      "[72]\ttrain-mae:0.002774\teval-mae:0.002781\n",
      "[73]\ttrain-mae:0.0025\teval-mae:0.002507\n",
      "[74]\ttrain-mae:0.002257\teval-mae:0.002264\n",
      "[75]\ttrain-mae:0.002042\teval-mae:0.002048\n",
      "[76]\ttrain-mae:0.001854\teval-mae:0.00186\n",
      "[77]\ttrain-mae:0.001693\teval-mae:0.001699\n",
      "[78]\ttrain-mae:0.001557\teval-mae:0.001562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[79]\ttrain-mae:0.001443\teval-mae:0.001447\n",
      "[80]\ttrain-mae:0.001348\teval-mae:0.001352\n",
      "[81]\ttrain-mae:0.00127\teval-mae:0.001273\n",
      "[82]\ttrain-mae:0.001206\teval-mae:0.001208\n",
      "[83]\ttrain-mae:0.001154\teval-mae:0.001155\n",
      "[84]\ttrain-mae:0.001111\teval-mae:0.001111\n",
      "[85]\ttrain-mae:0.001076\teval-mae:0.001075\n",
      "[86]\ttrain-mae:0.001047\teval-mae:0.001045\n",
      "[87]\ttrain-mae:0.001024\teval-mae:0.001021\n",
      "[88]\ttrain-mae:0.001006\teval-mae:0.001002\n",
      "[89]\ttrain-mae:0.000991\teval-mae:0.000987\n",
      "[90]\ttrain-mae:0.000979\teval-mae:0.000974\n",
      "[91]\ttrain-mae:0.000969\teval-mae:0.000965\n",
      "[92]\ttrain-mae:0.000962\teval-mae:0.000957\n",
      "[93]\ttrain-mae:0.000956\teval-mae:0.00095\n",
      "[94]\ttrain-mae:0.000951\teval-mae:0.000945\n",
      "[95]\ttrain-mae:0.000947\teval-mae:0.000941\n",
      "[96]\ttrain-mae:0.000944\teval-mae:0.000938\n",
      "[97]\ttrain-mae:0.000941\teval-mae:0.000935\n",
      "[98]\ttrain-mae:0.000939\teval-mae:0.000933\n",
      "[99]\ttrain-mae:0.000938\teval-mae:0.000931\n",
      "[100]\ttrain-mae:0.000937\teval-mae:0.00093\n",
      "[101]\ttrain-mae:0.000936\teval-mae:0.000929\n",
      "[102]\ttrain-mae:0.000935\teval-mae:0.000928\n",
      "[103]\ttrain-mae:0.000934\teval-mae:0.000927\n",
      "[104]\ttrain-mae:0.000934\teval-mae:0.000927\n",
      "[105]\ttrain-mae:0.000934\teval-mae:0.000926\n",
      "[106]\ttrain-mae:0.000933\teval-mae:0.000926\n",
      "[107]\ttrain-mae:0.000933\teval-mae:0.000926\n",
      "[108]\ttrain-mae:0.000933\teval-mae:0.000926\n",
      "[109]\ttrain-mae:0.000933\teval-mae:0.000926\n",
      "[110]\ttrain-mae:0.000933\teval-mae:0.000925\n",
      "[111]\ttrain-mae:0.000933\teval-mae:0.000925\n",
      "[112]\ttrain-mae:0.000933\teval-mae:0.000925\n",
      "[113]\ttrain-mae:0.000933\teval-mae:0.000925\n",
      "[114]\ttrain-mae:0.000933\teval-mae:0.000925\n",
      "[115]\ttrain-mae:0.000933\teval-mae:0.000925\n",
      "[116]\ttrain-mae:0.000933\teval-mae:0.000925\n",
      "[117]\ttrain-mae:0.000933\teval-mae:0.000925\n",
      "[118]\ttrain-mae:0.000933\teval-mae:0.000925\n",
      "[119]\ttrain-mae:0.000933\teval-mae:0.000925\n",
      "[120]\ttrain-mae:0.000933\teval-mae:0.000925\n",
      "[121]\ttrain-mae:0.000933\teval-mae:0.000925\n",
      "[122]\ttrain-mae:0.000933\teval-mae:0.000925\n",
      "[123]\ttrain-mae:0.000933\teval-mae:0.000925\n",
      "[124]\ttrain-mae:0.000933\teval-mae:0.000925\n",
      "[125]\ttrain-mae:0.000933\teval-mae:0.000925\n",
      "[126]\ttrain-mae:0.000933\teval-mae:0.000926\n",
      "[127]\ttrain-mae:0.000933\teval-mae:0.000926\n",
      "[128]\ttrain-mae:0.000933\teval-mae:0.000926\n",
      "[129]\ttrain-mae:0.000933\teval-mae:0.000926\n",
      "[130]\ttrain-mae:0.000933\teval-mae:0.000926\n",
      "[131]\ttrain-mae:0.000933\teval-mae:0.000926\n",
      "[132]\ttrain-mae:0.000933\teval-mae:0.000926\n",
      "[133]\ttrain-mae:0.000933\teval-mae:0.000926\n",
      "[134]\ttrain-mae:0.000933\teval-mae:0.000926\n",
      "[135]\ttrain-mae:0.000933\teval-mae:0.000926\n",
      "Stopping. Best iteration:\n",
      "[110]\ttrain-mae:0.000933\teval-mae:0.000925\n",
      "\n",
      " eval-MAE: 0.654943\n",
      "[0]\ttrain-mae:5.4558\teval-mae:5.4558\n",
      "Multiple eval metrics have been passed: 'eval-mae' will be used for early stopping.\n",
      "\n",
      "Will train until eval-mae hasn't improved in 25 rounds.\n",
      "[1]\ttrain-mae:4.91023\teval-mae:4.91022\n",
      "[2]\ttrain-mae:4.41921\teval-mae:4.4192\n",
      "[3]\ttrain-mae:3.97729\teval-mae:3.97728\n",
      "[4]\ttrain-mae:3.57956\teval-mae:3.57955\n",
      "[5]\ttrain-mae:3.22161\teval-mae:3.2216\n",
      "[6]\ttrain-mae:2.89945\teval-mae:2.89944\n",
      "[7]\ttrain-mae:2.60951\teval-mae:2.6095\n",
      "[8]\ttrain-mae:2.34856\teval-mae:2.34855\n",
      "[9]\ttrain-mae:2.1137\teval-mae:2.11369\n",
      "[10]\ttrain-mae:1.90233\teval-mae:1.90233\n",
      "[11]\ttrain-mae:1.7121\teval-mae:1.71209\n",
      "[12]\ttrain-mae:1.54089\teval-mae:1.54088\n",
      "[13]\ttrain-mae:1.3868\teval-mae:1.3868\n",
      "[14]\ttrain-mae:1.24812\teval-mae:1.24812\n",
      "[15]\ttrain-mae:1.12331\teval-mae:1.1233\n",
      "[16]\ttrain-mae:1.01098\teval-mae:1.01097\n",
      "[17]\ttrain-mae:0.909885\teval-mae:0.909877\n",
      "[18]\ttrain-mae:0.818897\teval-mae:0.818889\n",
      "[19]\ttrain-mae:0.737007\teval-mae:0.737\n",
      "[20]\ttrain-mae:0.663307\teval-mae:0.663299\n",
      "[21]\ttrain-mae:0.596977\teval-mae:0.596969\n",
      "[22]\ttrain-mae:0.537279\teval-mae:0.537272\n",
      "[23]\ttrain-mae:0.483552\teval-mae:0.483544\n",
      "[24]\ttrain-mae:0.435197\teval-mae:0.435189\n",
      "[25]\ttrain-mae:0.391677\teval-mae:0.39167\n",
      "[26]\ttrain-mae:0.35251\teval-mae:0.352502\n",
      "[27]\ttrain-mae:0.317259\teval-mae:0.317251\n",
      "[28]\ttrain-mae:0.285533\teval-mae:0.285526\n",
      "[29]\ttrain-mae:0.25698\teval-mae:0.256973\n",
      "[30]\ttrain-mae:0.231282\teval-mae:0.231275\n",
      "[31]\ttrain-mae:0.208154\teval-mae:0.208147\n",
      "[32]\ttrain-mae:0.187339\teval-mae:0.187331\n",
      "[33]\ttrain-mae:0.168605\teval-mae:0.168597\n",
      "[34]\ttrain-mae:0.151745\teval-mae:0.151737\n",
      "[35]\ttrain-mae:0.13657\teval-mae:0.136563\n",
      "[36]\ttrain-mae:0.122913\teval-mae:0.122906\n",
      "[37]\ttrain-mae:0.110622\teval-mae:0.110614\n",
      "[38]\ttrain-mae:0.09956\teval-mae:0.099552\n",
      "[39]\ttrain-mae:0.089604\teval-mae:0.089596\n",
      "[40]\ttrain-mae:0.080643\teval-mae:0.080636\n",
      "[41]\ttrain-mae:0.072579\teval-mae:0.072572\n",
      "[42]\ttrain-mae:0.065321\teval-mae:0.065314\n",
      "[43]\ttrain-mae:0.058789\teval-mae:0.058781\n",
      "[44]\ttrain-mae:0.05291\teval-mae:0.052902\n",
      "[45]\ttrain-mae:0.047619\teval-mae:0.047611\n",
      "[46]\ttrain-mae:0.042857\teval-mae:0.04285\n",
      "[47]\ttrain-mae:0.038572\teval-mae:0.038564\n",
      "[48]\ttrain-mae:0.034714\teval-mae:0.034707\n",
      "[49]\ttrain-mae:0.031243\teval-mae:0.031235\n",
      "[50]\ttrain-mae:0.028119\teval-mae:0.028111\n",
      "[51]\ttrain-mae:0.025307\teval-mae:0.025299\n",
      "[52]\ttrain-mae:0.022776\teval-mae:0.022769\n",
      "[53]\ttrain-mae:0.020499\teval-mae:0.020491\n",
      "[54]\ttrain-mae:0.018449\teval-mae:0.018441\n",
      "[55]\ttrain-mae:0.016604\teval-mae:0.016597\n",
      "[56]\ttrain-mae:0.014944\teval-mae:0.014936\n",
      "[57]\ttrain-mae:0.013449\teval-mae:0.013442\n",
      "[58]\ttrain-mae:0.012105\teval-mae:0.012097\n",
      "[59]\ttrain-mae:0.010894\teval-mae:0.010886\n",
      "[60]\ttrain-mae:0.009805\teval-mae:0.009797\n",
      "[61]\ttrain-mae:0.008824\teval-mae:0.008816\n",
      "[62]\ttrain-mae:0.007942\teval-mae:0.007934\n",
      "[63]\ttrain-mae:0.007148\teval-mae:0.00714\n",
      "[64]\ttrain-mae:0.006433\teval-mae:0.006425\n",
      "[65]\ttrain-mae:0.00579\teval-mae:0.005782\n",
      "[66]\ttrain-mae:0.005211\teval-mae:0.005203\n",
      "[67]\ttrain-mae:0.00469\teval-mae:0.004682\n",
      "[68]\ttrain-mae:0.004221\teval-mae:0.004214\n",
      "[69]\ttrain-mae:0.0038\teval-mae:0.003792\n",
      "[70]\ttrain-mae:0.003421\teval-mae:0.003412\n",
      "[71]\ttrain-mae:0.00308\teval-mae:0.003071\n",
      "[72]\ttrain-mae:0.002774\teval-mae:0.002766\n",
      "[73]\ttrain-mae:0.0025\teval-mae:0.002492\n",
      "[74]\ttrain-mae:0.002257\teval-mae:0.002248\n",
      "[75]\ttrain-mae:0.002042\teval-mae:0.002033\n",
      "[76]\ttrain-mae:0.001854\teval-mae:0.001846\n",
      "[77]\ttrain-mae:0.001694\teval-mae:0.001685\n",
      "[78]\ttrain-mae:0.001557\teval-mae:0.001549\n",
      "[79]\ttrain-mae:0.001443\teval-mae:0.001435\n",
      "[80]\ttrain-mae:0.001348\teval-mae:0.001341\n",
      "[81]\ttrain-mae:0.00127\teval-mae:0.001263\n",
      "[82]\ttrain-mae:0.001206\teval-mae:0.0012\n",
      "[83]\ttrain-mae:0.001153\teval-mae:0.001148\n",
      "[84]\ttrain-mae:0.001111\teval-mae:0.001105\n",
      "[85]\ttrain-mae:0.001076\teval-mae:0.00107\n",
      "[86]\ttrain-mae:0.001047\teval-mae:0.001042\n",
      "[87]\ttrain-mae:0.001024\teval-mae:0.001019\n",
      "[88]\ttrain-mae:0.001006\teval-mae:0.001001\n",
      "[89]\ttrain-mae:0.000991\teval-mae:0.000986\n",
      "[90]\ttrain-mae:0.000979\teval-mae:0.000975\n",
      "[91]\ttrain-mae:0.000969\teval-mae:0.000965\n",
      "[92]\ttrain-mae:0.000961\teval-mae:0.000958\n",
      "[93]\ttrain-mae:0.000955\teval-mae:0.000952\n",
      "[94]\ttrain-mae:0.00095\teval-mae:0.000947\n",
      "[95]\ttrain-mae:0.000946\teval-mae:0.000944\n",
      "[96]\ttrain-mae:0.000943\teval-mae:0.000941\n",
      "[97]\ttrain-mae:0.000941\teval-mae:0.000938\n",
      "[98]\ttrain-mae:0.000939\teval-mae:0.000937\n",
      "[99]\ttrain-mae:0.000937\teval-mae:0.000935\n",
      "[100]\ttrain-mae:0.000936\teval-mae:0.000934\n",
      "[101]\ttrain-mae:0.000935\teval-mae:0.000933\n",
      "[102]\ttrain-mae:0.000934\teval-mae:0.000933\n",
      "[103]\ttrain-mae:0.000934\teval-mae:0.000932\n",
      "[104]\ttrain-mae:0.000933\teval-mae:0.000932\n",
      "[105]\ttrain-mae:0.000933\teval-mae:0.000932\n",
      "[106]\ttrain-mae:0.000933\teval-mae:0.000932\n",
      "[107]\ttrain-mae:0.000933\teval-mae:0.000931\n",
      "[108]\ttrain-mae:0.000932\teval-mae:0.000931\n",
      "[109]\ttrain-mae:0.000932\teval-mae:0.000931\n",
      "[110]\ttrain-mae:0.000932\teval-mae:0.000931\n",
      "[111]\ttrain-mae:0.000932\teval-mae:0.000931\n",
      "[112]\ttrain-mae:0.000932\teval-mae:0.000931\n",
      "[113]\ttrain-mae:0.000932\teval-mae:0.000931\n",
      "[114]\ttrain-mae:0.000932\teval-mae:0.000931\n",
      "[115]\ttrain-mae:0.000932\teval-mae:0.000931\n",
      "[116]\ttrain-mae:0.000932\teval-mae:0.000932\n",
      "[117]\ttrain-mae:0.000932\teval-mae:0.000932\n",
      "[118]\ttrain-mae:0.000932\teval-mae:0.000932\n",
      "[119]\ttrain-mae:0.000932\teval-mae:0.000932\n",
      "[120]\ttrain-mae:0.000932\teval-mae:0.000932\n",
      "[121]\ttrain-mae:0.000932\teval-mae:0.000932\n",
      "[122]\ttrain-mae:0.000932\teval-mae:0.000932\n",
      "[123]\ttrain-mae:0.000932\teval-mae:0.000932\n",
      "[124]\ttrain-mae:0.000932\teval-mae:0.000932\n",
      "[125]\ttrain-mae:0.000932\teval-mae:0.000932\n",
      "[126]\ttrain-mae:0.000932\teval-mae:0.000932\n",
      "[127]\ttrain-mae:0.000933\teval-mae:0.000932\n",
      "[128]\ttrain-mae:0.000933\teval-mae:0.000932\n",
      "[129]\ttrain-mae:0.000933\teval-mae:0.000932\n",
      "[130]\ttrain-mae:0.000933\teval-mae:0.000932\n",
      "[131]\ttrain-mae:0.000933\teval-mae:0.000932\n",
      "[132]\ttrain-mae:0.000933\teval-mae:0.000932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping. Best iteration:\n",
      "[107]\ttrain-mae:0.000933\teval-mae:0.000931\n",
      "\n",
      " eval-MAE: 0.659196\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(train_data)):\n",
    "    #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_val = train_data.iloc[train_index], train_data.iloc[test_index]\n",
    "    y_train, y_val = Y.iloc[train_index], Y.iloc[test_index]\n",
    "\n",
    "    RANDOM_STATE = 2016\n",
    "    \n",
    "    params = {}\n",
    "    params['booster'] = 'gbtree'\n",
    "    params['objective'] = \"reg:linear\"\n",
    "    params['eval_metric'] = 'mae'\n",
    "    params['eta'] = 0.1\n",
    "    params['gamma'] = 0.5290\n",
    "    params['min_child_weight'] = 4.2922\n",
    "    params['colsample_bytree'] = 0.3085\n",
    "    params['subsample'] = 0.9930\n",
    "    params['max_depth'] = 7\n",
    "    params['max_delta_step'] = 0\n",
    "    params['silent'] = 1\n",
    "    params['random_state'] = RANDOM_STATE\n",
    "\n",
    "    d_train = xgb.DMatrix(X_train, label=y_train)\n",
    "    d_valid = xgb.DMatrix(X_val, label=y_val)\n",
    "    \n",
    "    watchlist = [(d_train, 'train'), (d_valid, 'eval')]\n",
    "    model = xgb.train(params, d_train, 2500, watchlist,early_stopping_rounds=early_stopping)\n",
    "    \n",
    "    xgb_rounds.append(model.best_iteration)\n",
    "    scores_val = model.predict(d_valid, ntree_limit=model.best_ntree_limit)\n",
    "    cv_score = mean_absolute_error(np.exp(y_val), np.exp(scores_val))\n",
    "    \n",
    "    print(' eval-MAE: %.6f' % cv_score)\n",
    "    \n",
    "    y_pred = np.exp(model.predict(d_test, ntree_limit=model.best_ntree_limit))\n",
    "    \n",
    "    if i > 0:\n",
    "        fpred = pred + y_pred\n",
    "    else:\n",
    "        fpred = y_pred\n",
    "    pred = fpred\n",
    "    cv_sum = cv_sum + cv_score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.59969458328 [ 7076.32617188  7076.32617188  7076.32617188 ...,  7076.32617188\n",
      "  7076.32617188  7076.32617188] (125546,)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Average eval-MAE: 0.659969\n",
      "[ 707.63262939  707.63262939  707.63262939 ...,  707.63262939  707.63262939\n",
      "  707.63262939]\n",
      "106\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training full dataset for 106 rounds ...\n"
     ]
    }
   ],
   "source": [
    "print('\\n Training full dataset for %d rounds ...' % n_rounds)\n",
    "watchlist = [(d_train_full, 'train')]\n",
    "model_full = xgb.train(params, d_train_full,n_rounds,watchlist,verbose_eval=False,)\n",
    "y_pred_full = np.exp(model_full.predict(d_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training full dataset for 413 rounds ...\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nprint np.isinf(Y_ans).any()\\nprint np.isinf(y_pred_fixed).any()\\nprint Y_ans\\nprint y_pred_fixed\\n'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ans_score = mean_absolute_error((Y_ans), (y_pred_fixed))\n",
    "#print ans_score\n",
    "\n",
    "#score is 2667.196 indicating we have to run it on larger scale to achieve good results\n",
    "#on 700 shift it is 2102 ,improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
