{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supress unnecessary warnings so that presentation looks clean\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas                                      #to read and manipulate data\n",
    "import zipfile                                     #to extract data\n",
    "import numpy as np                                 #for matrix operations\n",
    "#read will be imported as and when required\n",
    "#read the train and test zip file\n",
    "zip_ref = zipfile.ZipFile(\"train.csv.zip\", 'r')    \n",
    "zip_ref.extractall()                               \n",
    "zip_ref.close()\n",
    "\n",
    "train_data = pandas.read_csv(\"train.csv\")\n",
    "\n",
    "import copy\n",
    "test_data = copy.deepcopy(train_data.iloc[150000:])\n",
    "train_data = train_data.iloc[:150000]\n",
    "\n",
    "y_true = test_data['loss']\n",
    "\n",
    "ids = test_data['id']\n",
    "\n",
    "target = train_data['loss']\n",
    "\n",
    "#drop the unnecessary column id and loss from both train and test set.\n",
    "train_data.drop(['id','loss'],1,inplace=True)\n",
    "test_data.drop(['id','loss'],1,inplace=True)\n",
    "\n",
    "shift = 200\n",
    "target = np.log(target+shift)\n",
    "\n",
    "#merging both the datasets to make single joined dataset\n",
    "joined = pandas.concat([train_data, test_data],ignore_index = True)\n",
    "del train_data,test_data                                         #deleting previous one to save memory.\n",
    "\n",
    "cat_feature = [n for n in joined.columns if n.startswith('cat')]  #list of all the features containing categorical values\n",
    "\n",
    "#factorizing them\n",
    "for column in cat_feature:\n",
    "    joined[column] = pandas.factorize(joined[column].values, sort=True)[0]\n",
    "        \n",
    "del cat_feature\n",
    "\n",
    "train_data = joined.iloc[:150000,:]\n",
    "test_data = joined.iloc[150000:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/demonicode/.local/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from bayes_opt import BayesianOptimization\n",
    "from tqdm import tqdm\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xgb_evaluate(min_child_weight,\n",
    "                 colsample_bytree,\n",
    "                 max_depth,\n",
    "                 subsample,\n",
    "                 gamma,\n",
    "                 alpha):\n",
    "\n",
    "    params['min_child_weight'] = int(min_child_weight)\n",
    "    params['cosample_bytree'] = max(min(colsample_bytree, 1), 0)\n",
    "    params['max_depth'] = int(max_depth)\n",
    "    params['subsample'] = max(min(subsample, 1), 0)\n",
    "    params['gamma'] = max(gamma, 0)\n",
    "    params['alpha'] = max(alpha, 0)\n",
    "\n",
    "\n",
    "    cv_result = xgb.cv(params, xgtrain, num_boost_round=num_rounds, nfold=5,\n",
    "             seed=random_state,\n",
    "             callbacks=[xgb.callback.early_stop(50)])\n",
    "\n",
    "    return -cv_result['test-mae-mean'].values[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgtrain = xgb.DMatrix(train_data, label=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_rounds = 3000\n",
    "random_state = 2016\n",
    "num_iter = 25\n",
    "init_points = 5\n",
    "params = {'eta': 0.1,'silent': 1,'eval_metric': 'mae','verbose_eval': True,'seed': random_state}\n",
    "\n",
    "xgbBO = BayesianOptimization(xgb_evaluate, {'min_child_weight': (1, 20),\n",
    "                                                'colsample_bytree': (0.1, 1),\n",
    "                                                'max_depth': (5, 15),\n",
    "                                                'subsample': (0.5, 1),\n",
    "                                                'gamma': (0, 10),\n",
    "                                                'alpha': (0, 10),\n",
    "                                                })\n",
    "\n",
    "xgbBO.maximize(init_points=init_points, n_iter=num_iter)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
